---
title: Understanding Positional Embedding in Transformers
date: '2021-04-21'
template: 'post'
draft: true
tags:
- 'Machine Learning'
- 'Transformers'
- 'Attention'
- 'How To' 
description: 'How Attention was created? Why does it work and why it is one of the most important things in ML right now?'
---

## Let's abstract the Attention

<figure>
    <div  class="center-all" id="sin-position-embedding-diagram">
        <sin-position-embedding></sin-position-embedding>
    </div>
    <figcaption>Figure 1: Sequence-to-sequence with RNN, Designed base on <a href="https://arxiv.org/abs/1409.3215" target="_blank"><i>“Sequence to sequence learning with neural networks”</i>, NeurIPS 2014</a> Paper</figcaption>
</figure>
