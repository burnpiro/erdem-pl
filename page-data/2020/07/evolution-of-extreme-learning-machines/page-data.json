{"componentChunkName":"component---src-templates-post-template-js","path":"/2020/07/evolution-of-extreme-learning-machines","result":{"data":{"markdownRemark":{"fields":{"slug":"/2020/07/evolution-of-extreme-learning-machines","tagSlugs":["/tag/ai/","/tag/neural-networks/","/tag/machine-learning/"],"readTime":{"text":"25 min read","minutes":24.516666666666666}},"frontmatter":{"description":"How ELMs were evolving through the years and what is their status right now?","tags":["AI","Neural Networks","MachineLearning"],"date":"2020-07-23","title":"Evolution of Extreme Learning Machines"},"html":"<blockquote>\n<p>Note! This is just a major overview of the ELM evolution. It doesn’t include all possible versions and tweaks done to ELMs through the years.</p>\n</blockquote>\n<h2 id=\"what-is-elm\"><a href=\"#what-is-elm\" aria-label=\"what is elm permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What is ELM?</h2>\n<p>ELM (Extreme Learning Machines) are feedforward neural networks. “Invented” in 2006 by <em>G. Huang</em> and it’s based on the idea of inverse matrix aproximation.</p>\n<p>If you’re not familiar with ELMs please check out my article <a href=\"https://erdem.pl/2020/05/introduction-to-extreme-learning-machines\">“Introduction to Extreme Learning Machines”</a> first. </p>\n<h2 id=\"when-did-evolution-started\"><a href=\"#when-did-evolution-started\" aria-label=\"when did evolution started permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>When did evolution started?</h2>\n<h3 id=\"i-elm-2006\"><a href=\"#i-elm-2006\" aria-label=\"i elm 2006 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf\">I-ELM (2006)</a></h3>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 649px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 52.08012326656395%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsSAAALEgHS3X78AAABJUlEQVQoz22S6Y6DMAyEef9HTNhKZQXlTCAJ99H9SLZI265/RHbGY88EoqeP4zief8MYU1d10zQkn2igRPu+k23bZq1dty0kELIsa9sWcpamlCe6rgHlDOMirXVVVVLIvu9vXzfVNIzrneNclsV03UkwhpLOWMbjOJY+vpPk3AwM0zmnfQAURWH9Nu7pdtYWea6VAuVymiY2z/McXR4oOOu6Ri0dQog8z7Gd3O9AMKuypAHmRYlYy3L8YIztmMwfjyCE1b+znEML0HlpDHZgec9KsUEKgbxYSq3VNI5YvV6RpOu6YRjQTNvw8pzg+fARjDFyHIbe9XQjb6Wez+jaFi3wERienYmc0ecHDB1pmiIYGol7Ed7iH3IITENWSjnr3n6OK/0Bfsg/3X3CkcUAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"I-ELM\" title=\"I-ELM\" src=\"/static/4dfe4069e8612915871dc66dfe4ee15e/bc914/i-elm.png\" srcset=\"/static/4dfe4069e8612915871dc66dfe4ee15e/e4891/i-elm.png 240w,\n/static/4dfe4069e8612915871dc66dfe4ee15e/0ce91/i-elm.png 480w,\n/static/4dfe4069e8612915871dc66dfe4ee15e/bc914/i-elm.png 649w\" sizes=\"(max-width: 649px) 100vw, 649px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>I-ELM structure. Source: <a href=\"https://www.researchgate.net/publication/341365884_An_improved_algorithm_for_incremental_extreme_learning_machine\" target=\"_blank\">An improved algorithm for incremental extremelearning machine</a></figcaption>\n</figure>\n<p>After the original publication in 2006, Huang and his associates published another paper on a different type of ELMs called <a href=\"https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf\">I-ELM</a> (Incremental ELM). As the name says, I-ELM is an incremental version of the standard ELM network. Idea of I-ELM is quite simple:</p>\n<p>Define max number of hidden nodes <strong>L</strong> and expected training accuracy <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span></span></span></span>\nStarting from <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">l=0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">0</span></span></span></span> (l is a number of current hidden nodes):</p>\n<ul>\n<li>Increment <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>l</mi><mi>t</mi></msub><mo>=</mo><msub><mi>l</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l_t = l_{t-1} + 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.902771em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">t</span><span class=\"mbin mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span></li>\n<li>Initialize weights <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>w</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">w_l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> and bias <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>b</mi><mi>l</mi></msub></mrow><annotation encoding=\"application/x-tex\">b_l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">b</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> of the newly added hidden neuron randomly (do not reinitialize already existing neurons)</li>\n<li>Calculate output vector <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span></li>\n<li>Calculate weight vector <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span></li>\n<li>Calculate error after adding node</li>\n<li>Check if <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>&lt;</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">E &lt; \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span></span></span></span></li>\n<li>If not then increase the number of hidden nodes and repeat the process.</li>\n</ul>\n<p>There is a chance that <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>&gt;</mo><mi>L</mi></mrow><annotation encoding=\"application/x-tex\">l &gt; L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.73354em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">L</span></span></span></span> at some point in the process and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>E</mi><mo>&gt;</mo><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">E &gt; \\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72243em;vertical-align:-0.0391em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05764em;\">E</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&gt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span></span></span></span>. At this point, we should repeat the whole process of training and initialization.</p>\n<p>The idea of incrementing the size of the network is not new and usually produces better results than setting network size “by hand”. There is one disadvantage which is especially important in terms of ELMs… computation time. If your network happens to be large (let’s say 1000 hidden nodes), in worse cases we have to make 1000 matrix inversions.</p>\n<p>If you’re interested in I-ELM, you should know there are many variations of it:</p>\n<ul>\n<li>II-ELM (improved I-ELM)</li>\n<li>CI-ELM (convex I-ELM)</li>\n<li>EI-ELM (enhance I-ELM)</li>\n</ul>\n<p> I’m not going to explain every one of them because this article should be just a quick summary and a place to start instead of the whole book about all variations of ELMs. Besides that probably every person reading this is here not by a mistake and know how to find more information about an interesting topic if he/she knows what to look for :P</p>\n<h3 id=\"p-elm-2008\"><a href=\"#p-elm-2008\" aria-label=\"p elm 2008 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/publication/222429523_A_fast_pruned-extreme_learning_machine_for_classification_problem\">P-ELM (2008)</a></h3>\n<p>After introducing an incremental version of ELM another improvement was to use pruning to achieve the optimal structure of the network. P-ELM (pruned ELM) was introduced in 2008 by Hai-Jun Rong. The algorithm starts with a very large network and removes nodes that are not relevant to predictions. By “not relevant” we mean that node is not taking part in predicting output value (i.e. output value is close to 0). This idea was able to produce smaller classifiers and is mostly suitable for pattern classification.</p>\n<h3 id=\"em-elm-2009\"><a href=\"#em-elm-2009\" aria-label=\"em elm 2009 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/publication/26665344_Error_Minimized_Extreme_Learning_Machine_With_Growth_of_Hidden_Nodes_and_Incremental_Learning\">EM-ELM (2009)</a></h3>\n<p>This version of ELM is not a standalone version but an improvement of I-ELM. EM stands for Error-Minimized and allows to add a group of nodes instead of only one. Those nodes are inserted randomly into the network until the error is not below <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding=\"application/x-tex\">\\epsilon</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">ϵ</span></span></span></span>.</p>\n<h3 id=\"regularized-elm-2009\"><a href=\"#regularized-elm-2009\" aria-label=\"regularized elm 2009 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/publication/224453283_Regularized_Extreme_Learning_Machine\">Regularized ELM (2009)</a></h3>\n<p>Starting in 2009, Zheng studied the stability and generalization performance of ELM. He and his team come up with the idea of adding regularization to the original formula for calculating <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat\\beta</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span></span>.</p>\n<p>Right now it looks like:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi>β</mi><mo>^</mo></mover><mo>=</mo><msup><mrow><mo fence=\"true\">(</mo><mfrac><mn>1</mn><mi>C</mi></mfrac><mo>+</mo><msup><mi>H</mi><mi>T</mi></msup><mi>H</mi><mo fence=\"true\">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>H</mi><mi>T</mi></msup><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">\\hat\\beta = \\left (\\frac{1}{C}+H^TH \\right )^{-1} H^TT</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1523199999999998em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9578799999999998em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05278em;\">β</span></span><span style=\"top:-3.26344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\">^</span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.604038em;vertical-align:-0.95003em;\"></span><span class=\"minner\"><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.32144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.6540080000000001em;\"><span style=\"top:-3.9029000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8913309999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span>\n<h3 id=\"ts-elm-2010\"><a href=\"#ts-elm-2010\" aria-label=\"ts elm 2010 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://dl.acm.org/doi/10.1016/j.neucom.2010.07.012\">TS-ELM (2010)</a></h3>\n<p>Two-stage ELM (TS-ELM) was a proposition to once again minimize network structure. Like the name says, it consists of two stages:</p>\n<ol>\n<li>Applying forward recursive algorithm to choose the hidden nodes from candidates generated randomly in each step. Hidden nodes are added until the stopping criterion is matched.</li>\n<li>Review of an existing structure. Even if we created a network with the minimum number of nodes to match our criterion, some of them might no longer be that useful. In this stage, we’re going to remove unimportant nodes.</li>\n</ol>\n<h3 id=\"kelm-2010\"><a href=\"#kelm-2010\" aria-label=\"kelm 2010 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://dl.acm.org/doi/10.1145/2851613.2851882\">KELM (2010)</a></h3>\n<p>Kernel-based ELM (KELM) was introduced and uses kernel function instead of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi>H</mi><mi>T</mi></msup><mi>H</mi></mrow><annotation encoding=\"application/x-tex\">H^TH</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8413309999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8413309999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.13889em;\">T</span></span></span></span></span></span></span></span><span class=\"mord mathdefault\" style=\"margin-right:0.08125em;\">H</span></span></span></span>. This idea was inspired by SVM and the main kernel function used with ELMs is RBF (<a href=\"https://en.wikipedia.org/wiki/Radial_basis_function\">Radial Basis Function</a>). KELMs are used to design Deep ELMs.</p>\n<h3 id=\"v-elm-2012\"><a href=\"#v-elm-2012\" aria-label=\"v elm 2012 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/publication/220313291_Voting_based_extreme_learning_machine\">V-ELM (2012)</a></h3>\n<p>Voting-based ELM (V-ELM) was proposed in 2012 to improve performance on classification tasks. Problem was that the standard training process of ELM might not achieve the optimal boundary for classification then adding nodes randomly. Because of that, some samples which are near that boundary might be misclassified. In V-ELM we’re not training just one network but many of them and then, base on the majority voting method, selecting the optimal one.</p>\n<h3 id=\"elm-ae-2013\"><a href=\"#elm-ae-2013\" aria-label=\"elm ae 2013 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf\">ELM-AE (2013)</a></h3>\n<p>When in 2013 ideas like <a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">RBM</a> and <a href=\"https://en.wikipedia.org/wiki/Autoencoder\">autoencoders</a> starting to get popular, Kasnu produces a paper on ELM-AE (ELM Auto-Encoders). The main goal is to be able to reproduce an input vector, as well as standard autoencoders does. Structure of ELM-AE looks the same as standard ELM</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 743px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 82.50336473755047%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAACw0lEQVQ4y11UyY7TQBD1pyJxQkLixJUDBy5cEQckBAgxAokLjBDiAsNoQMphsq/ONrGd1Y4zSZw4E9t51OuMl6GlUtnd1a/fq+pq7XA4gGM4HKJUKuFqMECj2YTebiOStcOtxSP+DsMApXIZpUoVhnHc02y1oMUB9JvNBvv9PrHsCMNILEQUpQcw5kbM3wUIgkD9azHYbDbD9fU1bNtWi5ybz+dYLpdYr9cSFSkjMwUWHH29NcWDx5/xN1dDGNykgNy4Wq0UIIHJloC27cCeTXD6o4p3H3NwHBvT6RSb7Ra+vxGpfbz5cIZuz1SHJYC+7yvK9PzfyoZIJHKOgC9efceTZ59gWSZ6/b6Ko91mVlSJ/JsMw91up0DILs4nGZK14ziyvpX145zneXBdV2I8MR/dvg1vs1Wp0hjAak4mE0zGY0wll647TwpDsMVioYLJiN88dCkHWZaBs4sK7j86wfmfMtarBTRVOQFk4hnIAzj4TzDf32E6GeHl6594+vxUSTYtCxPJo+dJzHyJL99yGI1tUeHflUwfRZFiQ/nx1Vi4Dr5+v8Tbk9+iZIyBYUi+9iJ3m1yrKPzv2pARjRUkU+bLNE3Fci05SzZGofKUz5irwRAXuS5mtiuAmaIQiDaWPLJaMTvKG8kci8T8kbkn30FwBM6XDNx7+B6/zgsyt0sBOSg7vgqUTabVWg31RhN1aa18oYBisYjLfEHlkFclDA+yZkq3HElo2T7NDl6XZkuXPjVU5Xv9KwxHI5Fmo6XraDQasKT/s4NYCUNWOzZKpk/7OEweCRYttjSvaX9r8csxu2059rTe7qBcqaBSralXpFavoygvEeXHxrmVFDHu+zsM7wCKP26uQtfbAlxVoO1OR0ktSA473a6ymgB3ev2EbQoop9iqI1zMpaUu83kF0peeJQDfOeazXj8WiBWO5WffS/p/NogGDNfaJa4AAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"ELM-AE\" title=\"ELM-AE\" src=\"/static/9e5a43b56aa73e97584541f264723ad4/8bff5/elm-ae-2.png\" srcset=\"/static/9e5a43b56aa73e97584541f264723ad4/e4891/elm-ae-2.png 240w,\n/static/9e5a43b56aa73e97584541f264723ad4/0ce91/elm-ae-2.png 480w,\n/static/9e5a43b56aa73e97584541f264723ad4/8bff5/elm-ae-2.png 743w\" sizes=\"(max-width: 743px) 100vw, 743px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>ELM-AE structure. Source: <a href=\"https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf\" target=\"_blank\">Representational Learning with ELMs for Big Data</a></figcaption>\n</figure>\n<p>There are three types of ELM-AE:</p>\n<ul>\n<li>Compression. Higher-dimensional input space to the lower-dimensional hidden layer (less hidden nodes than input).</li>\n<li>Equal representation. Data dimensionality remains the same (same number of nodes in hidden and input)</li>\n<li>Sparsing. Lower-dimensional input space to the higher-dimensional hidden layer (more hidden nodes than input)</li>\n</ul>\n<p>There are two main differences between standard ELMs and ELM-AE. The first one is that ELM-AE is unsupervised. As an output, we’re using the same vectors as input. Second thing is that weights in ELM-AE are orthogonal, the same goes for bias in the hidden layer. This is important because ELM-AE is used to create a deep version of ELMs.</p>\n<h3 id=\"mlelm-2013\"><a href=\"#mlelm-2013\" aria-label=\"mlelm 2013 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf\">MLELM (2013)</a></h3>\n<p>In the same paper (Representational Learning with ELMs for Big Data) Kasnu proposed a version of ELM called Multi-Layer ELM. This idea is based on stacked autoencoders and consists of multiple ELM-AE.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 55.736714975845416%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsSAAALEgHS3X78AAAB2klEQVQoz21TCXLTQBDU/x9GAJOAwXKMj+iwDke3ZJ2ro5kZyU6ATNWWVFu9Pd09uxreled5ME0DjutC9b3sTdN0X+DvOGODIBSsZdvoum7G0tIYWBQFmqbBMAwwrBDFtUaW1/j6eIJSSsAjE1K9HEIcdr78206MMMpRVQ1WTwbqpoPGJExYVRXqusaPXwbiOKO9CifDf+u+KIzCKzwnQq9a6M82XC+iszVezIBEtdBYQZZlQlaWV6w3FiIizInwaW3IvigcR4zU3LZjnI6+EG6I0HEjwjQkxCZXxWyZCVlhT7m5fkKAjhQ20Hfnu+Vpsey7GSwjkMT81xQ5RcP4572LTvXQGNS2reQ3KxkwUPCdGmDaIXral6GMREiQwCkQXArBsoBhmOgMCBvNhLfOTNj3Ck074b+iA6Oav+9r+gB6J+Ss0iwnq1fsjxfsT/MkN2sXthlR4B0eVgY17GiaCp+/HZCmCXzflxvCGQuhSCd1ZVlSlvO07XOAy2vC/uHQRH03RpZm90m2RLo/nKHrOrbb7WJ9EFLtFjjnyOoYvKOAfx896fjzO03SSiTDh9VRlPL69GWHJEmooSPubpf/L8t5nkvI/Er65aUouofjOAfPTaflCt2m/2/9AZXPTuVBzk6jAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MLELM\" title=\"MLELM\" src=\"/static/68b5b7bdaacc9f02add0d93a32464b27/b7c40/mlelm2.png\" srcset=\"/static/68b5b7bdaacc9f02add0d93a32464b27/e4891/mlelm2.png 240w,\n/static/68b5b7bdaacc9f02add0d93a32464b27/0ce91/mlelm2.png 480w,\n/static/68b5b7bdaacc9f02add0d93a32464b27/b7c40/mlelm2.png 960w,\n/static/68b5b7bdaacc9f02add0d93a32464b27/5a2b3/mlelm2.png 1440w,\n/static/68b5b7bdaacc9f02add0d93a32464b27/923f8/mlelm2.png 1656w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>MLELM structure. Source: <a href=\"https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf\" target=\"_blank\">Representational Learning with ELMs for Big Data</a></figcaption>\n</figure>\n<p>You might ask “Why even bother with creating something similar to stacked autoencoders but with ELMs?“. If we look at how MLELM works we can see that it doesn’t require fine-tuning. That makes it a lot faster to construct than standard autoencoders networks. Like I’ve said, MLELM uses ELM-AE to train the parameters in each layer and removes output layers, so we’re left with only input and hidden layers of the ELM-AEs. </p>\n<h3 id=\"delm-2015\"><a href=\"#delm-2015\" aria-label=\"delm 2015 permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a><a href=\"https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification\">DELM (2015)</a></h3>\n<p>Deep ELM is one of the newest (and last major iteration in ELM evolution at the point of writing this article). DELMs are based on the idea of MLELMs with the use of KELM as the output layer. </p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 920px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 92.17391304347825%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsSAAALEgHS3X78AAACLElEQVQ4y1WUi9KqMAyEff939DKCcgehoIBg/n6p6fE4U1vTZLPZpB4+n4/s+y6Px0OappFhGHRP01TGcRTu+SzLIm3bStd14pxTP3yIw8f8DoA9n08PUsv9fpfb7SZFUei59Pu6LpoQkKqqJEmukue5+mRZpotk27YFQJDf77eC4ng+n9UZloAAhs/L38MqJE3VF3YwBgw/BeQLw7zMUte1ZD6Ave97mabxW87uz5MugLpv6QDiB6HIkC8Mdnk8HqUsS2UDQ9NmnmfVEOZJkmgVaAywscM3ApJ5UFaTAqENTA3w9Xppoq7zoN5u4EhgskQNyfT4yUQCQMkOED5UQMOsNHwtDv3/09A+NkKbB/wtw+zrukawfd808W+slkxGMpHFRsQcueOMfizu+T2OTuVgjNCyKLxcQ5BLAQEzzaxEZovGcMcyu3UbYGyQcW6MjVMNKQUwm7HJOyE4tuv1qtpxJpjdxqrvB9WWxBAg8QEwewWAENx7J7rOb3sN3BNMpymTLgManuIYGcemQLf2AADBBAACAABY59Qvump3ji5/B9veuwJC1bIBEN52o7a6rtSuAL4SdgA5k5hy2elFBKRsAzidTurAmfKxk+RyuWhAlt3jS2KnMtgHnOVfl8uyUCZ0ECdjQzJ8bKBNKxsvFnoSF5vC/xplWPl0kScGqL0aWHAHS35byYBRCdqDdYBNnocZtOYwpOFPtNfS6DJSVN9Sze5cGBvA2rbR+D8ofmnBYMibYQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"DELM\" title=\"DELM\" src=\"/static/ef145cb22b2c777d628f838a2dbd51c8/a1f45/delm.png\" srcset=\"/static/ef145cb22b2c777d628f838a2dbd51c8/e4891/delm.png 240w,\n/static/ef145cb22b2c777d628f838a2dbd51c8/0ce91/delm.png 480w,\n/static/ef145cb22b2c777d628f838a2dbd51c8/a1f45/delm.png 920w\" sizes=\"(max-width: 920px) 100vw, 920px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>DELM structure. Source: <a href=\"https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification\" target=\"_blank\">Deep Extreme Learning Machine and Its Application in EEG Classification.</a></figcaption>\n</figure>\n<h2 id=\"conclusion\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>ELMs were evolving through the years and definitely copying some major ideas from the field of machine learning. Some of those ideas work really great and could be useful when designing real-life models. You should remember that is just a brief summary of what happened in the field of ELM, not a complete review (not even close). It’s highly probable that if you type some prefix before ELM there is already a version of ELM with that prefix :)</p>\n<h3 id=\"references\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References:</h3>\n<ul>\n<li>Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew. “Extreme learning machine: Theory and applications”, 2006 <a href=\"https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-NC-2006.pdf\">Publication</a></li>\n<li>Guang-Bin Huang, Lei Chen, Chee-Kheong Siew. “Universal Approximation Using Incremental Constructive Feedforward Networks With Random Hidden Nodes”, 2006 <a href=\"https://www.researchgate.net/profile/Chee_Siew/publication/6928613_Universal_Approximation_Using_Incremental_Constructive_Feedforward_Networks_With_Random_Hidden_Nodes/links/00b4952f8672bc0621000000.pdf\">Publication</a></li>\n<li>Rong, Hai-Jun &#x26; Ong, Yew &#x26; Tan, Ah-Hwee &#x26; Zhu, Zexuan. (2008). A fast pruned-extreme learning machine for classification problem. Neurocomputing. <a href=\"https://www.researchgate.net/publication/222429523_A_fast_pruned-extreme_learning_machine_for_classification_problem\">Publication</a></li>\n<li>Feng, Guorui &#x26; Huang, Guang-Bin &#x26; Lin, Qingping &#x26; Gay, Robert. (2009). Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning. <a href=\"https://www.researchgate.net/publication/26665344_Error_Minimized_Extreme_Learning_Machine_With_Growth_of_Hidden_Nodes_and_Incremental_Learning\">Publication</a></li>\n<li>Wanyu, Deng &#x26; Zheng, Qinghua &#x26; Chen, Lin. (2009). Regularized Extreme Learning Machine. <a href=\"https://www.researchgate.net/publication/224453283_Regularized_Extreme_Learning_Machine\">Publication</a></li>\n<li>Lan, Y., Soh, Y. C., &#x26; Huang, G.-B. (2010). Two-stage extreme learning machine for regression. <a href=\"https://dl.acm.org/doi/10.1016/j.neucom.2010.07.012\">Publication</a></li>\n<li>Xiao-jian Ding, Xiao-guang Liu, and Xin Xu. 2016. An optimization method of extreme learning machine for regression. <a href=\"https://dl.acm.org/doi/10.1145/2851613.2851882\">Publication</a></li>\n<li>Cao, Jiuwen &#x26; Lin, Zhiping &#x26; Huang, Guang-Bin &#x26; Liu, Nan. (2012). Voting based extreme learning machine. <a href=\"https://www.researchgate.net/publication/220313291_Voting_based_extreme_learning_machine\">Publication</a></li>\n<li>Kasun, Liyanaarachchi &#x26; Zhou, Hongming &#x26; Huang, Guang-Bin &#x26; Vong, Chi-Man. (2013). Representational Learning with ELMs for Big Data. <a href=\"https://pdfs.semanticscholar.org/8df9/c71f09eb0dabf5adf17bee0f6b36190b52b2.pdf\">Publication</a></li>\n<li>Ding, Shifei &#x26; Zhang, Nan &#x26; Xu, Xinzheng &#x26; Guo, Lili &#x26; Zhang, Jian. (2015). Deep Extreme Learning Machine and Its Application in EEG Classification. <a href=\"https://www.researchgate.net/publication/277881335_Deep_Extreme_Learning_Machine_and_Its_Application_in_EEG_Classification\">Publication</a></li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>"}},"pageContext":{"slug":"/2020/07/evolution-of-extreme-learning-machines"}}}