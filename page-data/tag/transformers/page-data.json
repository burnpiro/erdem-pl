{"componentChunkName":"component---src-templates-tag-template-js","path":"/tag/transformers","result":{"data":{"site":{"siteMetadata":{"title":"Blog by Kemal Erdem","subtitle":"Even complex things could be explain in a simple way."}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"readTime":{"text":"54 min read","minutes":53.68333333333333},"slug":"/2021/05/introduction-to-attention-mechanism"},"frontmatter":{"date":"2021-05-12","title":"Introduction to Attention Mechanism","description":"How Attention was created? Why does it work and why it is one of the most important things in ML right now?"}}},{"node":{"fields":{"readTime":{"text":"17 min read","minutes":16.366666666666667},"slug":"/2021/05/understanding-positional-encoding-in-transformers"},"frontmatter":{"date":"2021-05-10","title":"Understanding Positional Encoding in Transformers","description":"Visualization of Positional Encoding method from Transformer models."}}}]}},"pageContext":{"tag":"Transformers","currentPage":0,"postsLimit":4,"postsOffset":0,"prevPagePath":"/tag/transformers","nextPagePath":"/tag/transformers/page/1","hasPrevPage":false,"hasNextPage":false}}}