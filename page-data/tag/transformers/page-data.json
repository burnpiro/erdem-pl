{"componentChunkName":"component---src-templates-tag-template-js","path":"/tag/transformers","result":{"data":{"site":{"siteMetadata":{"title":"Blog by Kemal Erdem","subtitle":"Even complex things could be explain in a simple way."}},"allMarkdownRemark":{"edges":[{"node":{"fields":{"readTime":{"text":"15 min read","minutes":14.85},"slug":"/2021/05/understanding-positional-encoding-in-transformers"},"frontmatter":{"date":"2021-05-10","title":"Understanding Positional Encoding in Transformers","description":"How Attention was created? Why does it work and why it is one of the most important things in ML right now?"}}},{"node":{"fields":{"readTime":{"text":"35 min read","minutes":34.166666666666664},"slug":"/2021/05/introduction-to-attention-mechanism"},"frontmatter":{"date":"2021-05-09","title":"Introduction to Attention Mechanism","description":"How Attention was created? Why does it work and why it is one of the most important things in ML right now?"}}}]}},"pageContext":{"tag":"Transformers","currentPage":0,"postsLimit":4,"postsOffset":0,"prevPagePath":"/tag/transformers","nextPagePath":"/tag/transformers/page/1","hasPrevPage":false,"hasNextPage":false}}}