{"componentChunkName":"component---src-templates-post-template-js","path":"/2021/02/animal-recognition-with-siamese-networks-and-mean-embeddings","result":{"data":{"markdownRemark":{"fields":{"slug":"/2021/02/animal-recognition-with-siamese-networks-and-mean-embeddings","tagSlugs":["/tag/machine-learning/","/tag/siamese-networks/","/tag/cnn/","/tag/recognition/","/tag/detection/"],"readTime":{"text":"36 min read","minutes":35.35}},"frontmatter":{"description":"Is Siamese Network a reliable solution for animal recognition? How Mean Embeddings can help with a few shot \"learning\"? Are we able to beat standard classification methods?","tags":["Machine Learning","Siamese Networks","CNN","Recognition","Detection"],"date":"2021-02-25","title":"Animal recognition with Siamese Networks and Mean Embeddings"},"html":"<p>When hearing about <a href=\"https://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\">Siamese Networks</a> you probably think about “Face Recognition”. That’s the most common use of those types of networks. We were trying to do sth else, recognize animals based only on top-view camera footage.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.57142857142857%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABhmbZFAWf/8QAGhABAQACAwAAAAAAAAAAAAAAAQIAEwMRQf/aAAgBAQABBQKGkvsN3JnlLrkHP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABkQAAMAAwAAAAAAAAAAAAAAAAABEQIhMv/aAAgBAQAGPwKvJlrOhFNn/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAERITFBUf/aAAgBAQABPyFTaayVSjjIehVANJK8Eq0lzs//2gAMAwEAAgADAAAAEJP/AP/EABYRAQEBAAAAAAAAAAAAAAAAAAABIf/aAAgBAwEBPxCMf//EABgRAQADAQAAAAAAAAAAAAAAAAEAETFR/9oACAECAQE/EG72A9n/xAAcEAEAAwACAwAAAAAAAAAAAAABABExIXFBUWH/2gAIAQEAAT8QtQaU4B6yAYwbbS++ZjsHFBajVJFdO2PJt+XyEZ1tX4J//9k=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Dataset example\" title=\"Dataset example\" src=\"/static/0d1967ca79d322579eadfe7082f026e2/0781d/example.jpg\" srcset=\"/static/0d1967ca79d322579eadfe7082f026e2/5a194/example.jpg 240w,\n/static/0d1967ca79d322579eadfe7082f026e2/8a430/example.jpg 480w,\n/static/0d1967ca79d322579eadfe7082f026e2/0781d/example.jpg 960w,\n/static/0d1967ca79d322579eadfe7082f026e2/3a5dd/example.jpg 1400w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 1: Frame from the dataset, Source: <a href=\"http://psrg.unl.edu/Projects/Details/12-Animal-Tracking\" target=\"_blank\">PSRG Dataset</a></figcaption>\n</figure>\n<p>If you’re not interested in “intro” then skip to <a href=\"#siamese-network-training\">Siamese Network Training</a>.</p>\n<h3 id=\"why-siamese-network\"><a href=\"#why-siamese-network\" aria-label=\"why siamese network permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why Siamese Network?</h3>\n<p>As you might know, Siamese Networks are widely used in recognition tasks. The main advantage over standard classification is that we can use one picture of the object we want to be able to recognize, and it just works. It’s not that simple of course, but the idea is not to retrain the whole network every time we want to add another class (person or animal or sth else). The basic structure of the Siamese Network looks like that:</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61.48514851485148%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsSAAALEgHS3X78AAAC0UlEQVQoz21TbUhTYRS+2yxqJqWCqzQYpASOQiRkBP2x/qx/EQX9G5gpi0XkRyTBUOfS1NFV0WtaTXJuYxvTTdNNS9ZckYT4S0RUVLQ5m19za84lb+dcr2bUgZf3PM95ee75upS2rrxAqy0rqKkuzWdaah7ae/SXKLCVlRV5MBhsXF9fFwOMk0qlYo1Gcxn8Mzwe7xi+kclklMfjQZdyu93sTTXWq0injiGtLZXE0aMjzn7zY+Sj0WgVAdve3n6GGNzUzc3NKoozmqaR48OhuPhewGFjdj66+nd6ui0Rm5UhXfrmJ8iPjY01oODk5KQGIE+lUl2dnp5uAF8kFAoTqEN2IIb29XM3ZKUjw8Nu4vn0gZiMzUXILy8vsxkGAoFKxF6vN3N1dVUL7gk4yRASQkto4CQYd7lcfFZw0Gnw9/a+8VvNLd8N+raoycDkI7+1FXyOgru7u3LEcIugn3ngHkVRCJ2CFkQjkYjsnywPg9zcCzy84eH5kpKSa+CmicVi/n48PT39OGZoMpnOzczM+Kampm4APpmRkZHEPrAa20Wv28pE3i+DKe8HzKdra9XxnGBSTk5OFrhns7Oz4/YFJRIJTjhVoVBkzc7OBqAVtwAnHQjq2rQx4zsm5rDrfnpGHMRiefsU+VAopCF7dpOrIhlKvAPuETjxgBN8Pt8P+PD1v6pkmirIQN8QMXS9+mWztROmVVuB/NLSIttDv99fjdjpdF6E3WwEN5EbSmI4HF7c2NhgBUdHRwWsoLmzI9bXPRAb8X6L1L+oIorCPDXy4+PjNApCj9gpq9XqK1AiTjkFTgKEeHNzc5nA4dQpWKm9DPUdXYTWNpCXdTRpqqdJYb68HPlwKKTmFruCKykN1qSW+o/Nz8//Affz7lqUCoWl8J7cWFz0wFla/Og29+spoY/WtbU1ds+USqXIbrdLcWUEAgE79YmJCf7CwgKrA9my929Ka6Q1CThEPQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Siamese structure\" title=\"Siamese structure\" src=\"/static/49f31b0aad06fe26399131e67026c4d9/b7c40/siamese.png\" srcset=\"/static/49f31b0aad06fe26399131e67026c4d9/e4891/siamese.png 240w,\n/static/49f31b0aad06fe26399131e67026c4d9/0ce91/siamese.png 480w,\n/static/49f31b0aad06fe26399131e67026c4d9/b7c40/siamese.png 960w,\n/static/49f31b0aad06fe26399131e67026c4d9/2bb2b/siamese.png 1010w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 2: Simple Siamese Network structure</figcaption>\n</figure>\n<p>The first part of the network is simple, and it’s just a basic feature extractor (ResNet or any other CNN-like network). Far more important is the loss function at the end. Currently, most of the SOTAs are using <a href=\"https://arxiv.org/abs/1412.6622\">Triplet Loss</a> but the basic idea is to maximize the distance between different embeddings (encoded images from CNNs). It’s just a brief summary of the Siamese Networks. I really encourage you to read the <a href=\"https://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\">original paper</a> and the bit on Triplet Loss to get a better understanding.</p>\n<h3 id=\"how-recognition-works-irl\"><a href=\"#how-recognition-works-irl\" aria-label=\"how recognition works irl permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How recognition works IRL?</h3>\n<p>In Real Life, we’re usually using images from the same device, taken under the same conditions, etc. Usually, that camera is in front of the security gate or somewhere where we’re sure that every picture is more or less similar. But that’s not the case when trying to recognize animals on the farm. Our study was dealing with multiple different cameras and lighting conditions (some of them were night videos). Because of that, we had to use two networks instead of just one. First to detect subjects and the second to recognize them.</p>\n<h3 id=\"detection-and-cropping\"><a href=\"#detection-and-cropping\" aria-label=\"detection and cropping permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Detection and cropping</h3>\n<p>Detection was quite straightforward and didn’t require a lot of work (except the annotations for training). We’ve taken standard SSD FPN Network with ResNet50 backbone and retrained it on our dataset.</p>\n<figure>\n<div class=\"center-all\">\n<table>\n<thead>\n<tr>\n<th></th>\n<th><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>A</mi><mi>P</mi></mrow><annotation encoding=\"application/x-tex\">mAP</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">A</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span></span></span></span></th>\n<th><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>A</mi><msub><mi>P</mi><mn>50</mn></msub></mrow><annotation encoding=\"application/x-tex\">mAP_{50}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">A</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">5</span><span class=\"mord mtight\">0</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></th>\n<th><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>m</mi><mi>A</mi><msub><mi>P</mi><mn>75</mn></msub></mrow><annotation encoding=\"application/x-tex\">mAP_{75}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord mathdefault\">m</span><span class=\"mord mathdefault\">A</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">7</span><span class=\"mord mtight\">5</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>SSD ResNet50 FPN</td>\n<td>72.92</td>\n<td>97.03</td>\n<td>81.82</td>\n</tr>\n</tbody>\n</table>\n</div>\n<figcaption>Table 1: mAP score</figcaption>\n</figure>\n<p>We’re not going to focus on the detection model because it’s just an extra to what we have to do. You can read more about training <a href=\"https://github.com/burnpiro/farm-animal-tracking/wiki/Detection-training\">on our GH Wiki</a>.</p>\n<h3 id=\"siamese-network-training\"><a href=\"#siamese-network-training\" aria-label=\"siamese network training permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Siamese Network Training</h3>\n<p>Here where the magic happens :) As I’ve mentioned before, we’re using <a href=\"https://arxiv.org/abs/1412.6622\">Triplet Loss</a> as our loss function. In our experiments, we were using three different baseline networks and then attaching custom layers on top of them. Those networks were:</p>\n<ul>\n<li>ResNet101 <a href=\"https://github.com/burnpiro/farm-animal-tracking/blob/main/assets/ResNet101V2_model_fig.png\">Siamese Network Structure</a></li>\n<li>EfficientNetB5 <a href=\"https://github.com/burnpiro/farm-animal-tracking/blob/main/assets/EfficientNetB5_model_fig.png\">Siamese Network Structure</a></li>\n<li>MobileNetV2 <a href=\"https://github.com/burnpiro/farm-animal-tracking/blob/main/assets/MobileNetV2_model_fig.png\">Siamese Network Structure</a></li>\n</ul>\n<p>We didn’t just attach our custom layers on top of the last layer as people usually do. Our idea was to add them somewhere in the middle to use more fine-grained features instead of high-level ones.</p>\n<ul>\n<li>ResNet101 - attached to the output of <em>layer4</em></li>\n<li>EfficientNetB5 - attached to the output of <em>layer3</em></li>\n<li>MobileNetV2 - attached to the output of <em>layer10</em></li>\n</ul>\n<p>Now when we’re done with the network structure we have to discuss the training process and how to prepare the data. We’re using Tensorflow and their Triplet Loss method, which makes it easier to use but also requires us to prepare our batches in a specific way. This particular loss function selects positive and negative examples from the given batch. Because of that with 16 different animals, we have to create a batch size with <strong>at least 2 examples from each class</strong>. That makes the <strong>minimum batch size to be 32</strong> (we’ve used 64). Second thing is that for every epoch we have to manually shuffle the batches to make sure that a minimal number of class examples is in every batch. Each manual shuffle, shuffles images per class and then merging shuffled datasets (dataset per class) into one and split them by given batch size.</p>\n<figure class=\"image\">\n  <img src=\"/af0e65fefaf585cbfa7d6cdea6af44de/siam_training.gif\" alt=\"Siamese training\">\n  <figcaption>Figure 3: Data batching and training process</figcaption>\n</figure>\n<p>Figure 3 is showing the simpler version of what I’ve just described. Notice that every batch has at least two inputs with the same class. Usually, we’re not going to use batch size 10, but it will do as an example.</p>\n<p>Our training script is <a href=\"https://github.com/burnpiro/farm-animal-tracking/wiki/Siamese-Network-Training-and-Evaluation\">available on GitHub</a>.</p>\n<h3 id=\"mean-embedding-approach\"><a href=\"#mean-embedding-approach\" aria-label=\"mean embedding approach permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Mean Embedding approach</h3>\n<p>Usually, we’re comparing an image with some anchor (true subject image). That works because of the same conditions. With multiple views and lightning conditions that just won’t work. We might try to compare the image with multiple images of the same subject, which might cause your script to slow down because you have to check every condition. Our idea was to use already generated embeddings and create mean embedding for every class.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 921px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 90.77090119435395%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsSAAALEgHS3X78AAACmElEQVQ4y5VUS08TURid+5jOTGemM51Hpx1aWiiFhkeLqQtFCU0LkfqMwSgGgo/IgkRCrC6IiY8liQtNdEtiWLhy596dG2PiTzrOTMtLW5DFzc2d75tzz3e+811BEASctmTO0L5Tx2jWjs6EnJjfP0i6u6+r+PVuHW/XmtGZkk6MUHIyYG5rEwOvXv6TFBNlLJ8fQX3E7wBSejpDc2UZzd8/0fjxHWyiDJFzCIxFMU44HCOBhCJ3LuAUXjaNRqMOKSb1BrTXVjC7+wXNz3vQywXIEgMJQUNGAkFMkkFlBQKh8CUVa6tL2Pv2FXFD6+pKepS8vgo3m0e+Oonmowew/NxBTAyWnnSQnarC8jzsfHyPnU8f0Ji/BNMye2h4eAMsQ0d1YQG6k0LRTiKTsCDHFZhmEkOTVaRSGTx7sY3n7TYyrhc0h/brMjlKPSpvuTKF1sgobD+DuKag2LqK2sZT0JgCLsdhWAokmf+fbRhlgegiMnoSY4Uh6MUScuUKnME8hOB7aJ3Fe3UMDKe7BI4A0i4zV1UwPpiDLIsBGI9+Krgurs/dgpUrgRGGGA3BSFQqpax3l1l3H3dtPJ6dg2sFUyFy+LUCDD+Jc4UKPCcNEtokZMKCMkMwwiKpjgESKmFmZhXjlZvQjHRUZsggVvRgNyYgKB2NREWB5jhQtQR4P6n2m7B4exuzrU0YqhqU0b2RHZ+IAc+GZ6rQS9Nw5u+ePCmcqwHTGPgRC5C/ki9MlpBN2eC6BX368tkeB0WUwFiH6b7PNNtCulyEJgaNkYKp6T3TfQADnz1p3Ifv+ofTosdR2ViCVxsGkejZGEpcxJsbW6jlpg8nKCHj2m4bV14/ROZi+ezvYcfc/MC0oabmWBqpWh6SpfXM/wNYpqia6N7nTAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Siamese embedding space\" title=\"Siamese embedding space\" src=\"/static/51fdf7c2362c8b8313a8afca28da6176/19c1a/emb-space.png\" srcset=\"/static/51fdf7c2362c8b8313a8afca28da6176/e4891/emb-space.png 240w,\n/static/51fdf7c2362c8b8313a8afca28da6176/0ce91/emb-space.png 480w,\n/static/51fdf7c2362c8b8313a8afca28da6176/19c1a/emb-space.png 921w\" sizes=\"(max-width: 921px) 100vw, 921px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 4: Sample embedding space. Every dot represents the image&apos;s embedding vector, every color is assigned to a different class</figcaption>\n</figure>\n<p>We can visualize 64D embedding in 3D space using <a href=\"https://arxiv.org/abs/1802.03426\">UMAP</a> with the help of <a href=\"https://projector.tensorflow.org/\">TensorFlow Embedding Projector</a>. You’ve noticed how different classes (colors) are clustered together. Fig. 4 is just an example, but we have to visualize space like that for every Siamese Network we’ve trained.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 83.14065510597302%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAsSAAALEgHS3X78AAABcklEQVQ4y61TyU4CQRCdXqeZHTMyAmGAgIyicU2M8aJEEznh0ZMHf8tgol/6bJpFJ2EZ0UOnq2qq37x6VWVZloVlRykbURSC2AIWWZ6z4qz+aPsu/GrF2KPRCGEY/g3QIsTcgR/gbTxGp9s1PpnF1wJGXgmc0SkzJXOPiEXx/vGJu8HA+JTSzYAPlz2UBMf+cQsXN/3pQ87MHTslXPUPECdJ8ZIDR4FqVn7oQkpuYjz24ZY93HfakJSAKZ3DOASnaFbj4hpSx5naWoJy4OF0L8lpJ3UlvbRSDJDqcRHFStPaFgAkTOvGWC7GdRMoJ6bUiX9S20Wr7G8xNvOfEKpBudF44r9cn+Oslqxi+aNczazSrIMJnktKkiNIbi/8xk4EJcVmhkQ34fDpFsqbNUUI0EZbz+X3hlApTXy7TRES7PnVbIwUDFLvtKjX4LRbWwJyDtLLFiu4XmdSvCm/PBsS1rCbs+pkGR6Hw/9j6Lgu0jQ19hc0U2eHRGRdHgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet embedding space\" title=\"MobileNet embedding space\" src=\"/static/11b3219bb9587a8c066bf821a753eb17/b7c40/MobileNetV2-layer10.png\" srcset=\"/static/11b3219bb9587a8c066bf821a753eb17/e4891/MobileNetV2-layer10.png 240w,\n/static/11b3219bb9587a8c066bf821a753eb17/0ce91/MobileNetV2-layer10.png 480w,\n/static/11b3219bb9587a8c066bf821a753eb17/b7c40/MobileNetV2-layer10.png 960w,\n/static/11b3219bb9587a8c066bf821a753eb17/805b2/MobileNetV2-layer10.png 1038w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 6: Train Emb. space for MobileNetV2 base Siamese Network</figcaption>\n</figure>\n<p>Network based on MobileNetV2 produced the best results, so we’re going to start from it. If we compare Fig. 6 and Fig. 5, clusters are even more separable with one cluster of “outliers” (bottom right corner). That extra cluster has all the outliers which because of minimizing Loss Function were separated from other clusters (unfortunately sometimes is easier for the model to sacrifice some of the examples to minimize loss). That is going to cause a problem soon but for now, we can use the current embeddings.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 78.80733944954127%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAAA3klEQVQ4y62U2wrCMAyGc2jWnVxh7kJE8U58IPH93yR2CjuWbZ1eBEoIX/P/SQsAoDGRMOqjKZdq4oCMoHUmvwEPJou5dL2IAOOBFZNebOLPqBjh57m0YSD7sIgfIBArICmLrHePWyV7ILDxcNJU6hjZgSTLCMxG9gGNMcEiEVCkQc5636Z2eLvQVWOgBDxjCvo0jzzT9PXsgRiQ1IIY4xZ/1OEUOutMaBvwaJ3eq+tgkb8xH1Z4MCZNNcnyHuik0Ftx6goKvyqOabNMaYF5vu9z+Mtb7jxu/Cdhljt/A8YOQw+AHUkTAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet embedding space avg\" title=\"MobileNet embedding space avg\" src=\"/static/88cefe24933e0ffbfadaa7e40c7a1c71/b7c40/MobileNetV2-avg.png\" srcset=\"/static/88cefe24933e0ffbfadaa7e40c7a1c71/e4891/MobileNetV2-avg.png 240w,\n/static/88cefe24933e0ffbfadaa7e40c7a1c71/0ce91/MobileNetV2-avg.png 480w,\n/static/88cefe24933e0ffbfadaa7e40c7a1c71/b7c40/MobileNetV2-avg.png 960w,\n/static/88cefe24933e0ffbfadaa7e40c7a1c71/e8a0c/MobileNetV2-avg.png 1090w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 7: Train Mean Emb. space for MobileNetV2 base Siamese Network</figcaption>\n</figure>\n<p>Fig. 7 shows the different version of the embedding space. In this case, we’ve calculated the average position of all examples from the given class. There is no “outliers” cluster anymore and each class has only one point (vector to be precise) in the space. With that mean values for the whole class, we can calculate the distance from the test image to that class and take the closest one as a predicted class.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEfElEQVQ4y6VUW0wcVRievbN3WK5yaQtxNzb1EqtgUUuhrLC3mV0WZmd2dnf2AqQUhKVIuQkFXHYrF2kMFpPatBqN8UWjDyYm+qSWeTEhJrXS+EjU2Afjk5fNkt9zzqy0T/rgSb7858zk//L9//efQwH4Kbw+uFmreHWEVwUrX1T21SRVfXX9quFj80qbptJiVJmrMMrUFdaFh7eUwdqYqlszrPxhr0Hx409VCpy/v19NHS6AMur/rN/u2Qipw+HAZMeV+HB3r8yfSXLvnFUMbjFWYZuxitu0JbZNm6NXGXPsKk0QJfCXRq6fppIXf7nbePaPe3XdhHS/nvBQnyz3qnDMxE5vvrfmBv6hODAlSQha+6HH0g8BYwIhCQFTshgTELIOwBkq9d03dxqnb+/XZYign+vVRGFKnyWEz1GRlQk2DuIF/ne3PpJHivJIYd5rjOR9pgiJxf1fjCUGnVT86xu7raM3v396Hud//OszKkLI20bU+IPH1JftqV8AcTCaT4Vo8Ori4DUJ4DUgGDEiRQgHjFmEbiomTe8Ex+duB+Zw/ty3PWq73U5RyfJpotBlZHOcfRZ6K5P58BAHdEcMvMoI+CwI5ughIVJ44LfGwYcI+744lw5+ObCI84W9MRUhTFXOEMJuQygXfnQemLKBvKcqBsnJILCP8eDRxIC2RgGZQYAJGUscDZsgnfpoarz585lZnN/y6RSplEpVzxFCjzmc5R2zwFSl8h5VGPpOCMBPh4GuQOXqEZE5Qgg9+vABbRIhQHGS/e3ldNP7S5dwftNnm3IPnze5tfiD28ivcEdfAro0jhovgEslQrSdhYWL7eAz/FOu8IDCkNT4+uV0w7W1l3H+kbc2ZMKnDG06QmjgM1zTFDC2RN6Hkv1lIriUIiymzsJSsg2clAiMRcBKkSkyoT2znm5a3SAuN13ZkgnHKnLFHrK5UF0a9TCRx67iEj3IYY82Cq9MtMF5jxdeoOJAW2RChmKlE5Pr6UfmZMLji2/IpiRKpwhhl57NsbVjQCNC1CdUWhRcJRy4tAL4ywUYXmaAdbDgUuOSRaAVIenJwfX04yOvkZKfGLkiKzxfvnRfYcOErNAgkFFB/UIR9ROp5Bo5eHfjJIRqQgc+Qz/4tSHpFLuWbgmvElNa42/KCjnzMLG7q4TNhmpGgUGm3B9kebCxGU4qCtNOGm5MtyJCNKNqTuroyo63uVbJ2LR15eSxYXRJsulUBTO9lcNo5sQ/PYZwAREWUOmFbi1XcJfw6MwXnFS8kIm25zeH2tFNEXa6Tl4aczYvkB66mi/LJUdsabU8Ntxa5NgMBGwp8BvRnFkSwKDo1UXBb0KuYiPQ2adPwLULnTAbce6eKZ+YdB0ZIzfF1TAuX72wbVTuoZEdChwduUOXJ75CDks+S3THYwxLqI+3UNzxmgXJaw5L7hLhVqCa3819+Ox1ppFnO9UxUTZVlBU+sLBSE4IRQVfc6xFKEazFPf6nQdDWtei0Kxo5sWMzSx0+sP+2HOeqKaVGYVablNr/fLYBSPgblxbbIn3YopsAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet conf matrix\" title=\"MobileNet conf matrix\" src=\"/static/27581d58ab4784169c2e0164cde7f007/b7c40/conf_matrix_MobileNetV2.png\" srcset=\"/static/27581d58ab4784169c2e0164cde7f007/e4891/conf_matrix_MobileNetV2.png 240w,\n/static/27581d58ab4784169c2e0164cde7f007/0ce91/conf_matrix_MobileNetV2.png 480w,\n/static/27581d58ab4784169c2e0164cde7f007/b7c40/conf_matrix_MobileNetV2.png 960w,\n/static/27581d58ab4784169c2e0164cde7f007/5a2b3/conf_matrix_MobileNetV2.png 1440w,\n/static/27581d58ab4784169c2e0164cde7f007/eff2e/conf_matrix_MobileNetV2.png 1920w,\n/static/27581d58ab4784169c2e0164cde7f007/cc694/conf_matrix_MobileNetV2.png 3000w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 8: Confusion Matrix for pig classification (MobileNetV2 base Siamese Network)</figcaption>\n</figure>\n<p><strong>F1-score</strong> for this approach is <strong>0.91</strong> which puts us way above the classifier we’ve used previously (F1 ~ 0.6).</p>\n<h3 id=\"problem-with-the-mean\"><a href=\"#problem-with-the-mean\" aria-label=\"problem with the mean permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Problem with the mean</h3>\n<p>Looking at the score itself is not enough because there is a small detail we might miss. You probably remember our “outliers” cluster from the Fig. 6. That outlier exists also in the test dataset and because we’re classifying examples base on the distance to the closest mean embedding, it results in <strong>classifying all the outliers as “Robert”</strong> (Robert’s mean is the closest one to the outliers, check Fig. 8 and Robert’s column). We could fix that by adding another mean, called “outliers” but then we have to define how to assign examples to the class “outliers” (we could use clusterization metrics).</p>\n<h3 id=\"another-network-bases\"><a href=\"#another-network-bases\" aria-label=\"another network bases permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Another network bases</h3>\n<p>As I’ve said, MobileNetV2 base was the best one to suit our problem. We’ve tried different approaches and <strong>ResNet101</strong> was a quite interesting one (<strong>F1 = 0.54</strong> on classification using mean embedding approach).</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 80.35714285714286%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAABmklEQVQ4y6WTyU7DMBCG4yWOnb0spQttU2gpixASBSGWUlVFSMCBI1wQ4ox4BR6CF/6ZlLAKpaEcRrbG9jczv2csy7JQxBhjbyvnH/ufliRJuhYDfoVOsekgJRV6nR5cRyGMS7C1zgs4HRh4IUbDEcrVCi4e7lFttWfP0HEkPMpwu9PG4e4GmDGI/BBa2H8HKqmxv9xDRCU2lhbRqc6BkV9wBYvJYsAk8NB0NSJbYiWO8DwYYnO1g6DenJyHboRyME97Bi5EPrCmNF5urzHuttA/7WNtrY6Ka7DgRQj9eXTbxzg6uAGXupiGXHBUjEJiNLTnISgFMNqgf3mFg6dHxI0ulHLBbQWb2+AWywEKOuRvF0QKpxaQxoFNj0/OxtgaDkg38r/rS8HzM8xgTAoyuuwbWPK7RqFn0I4N7s73UJsLiv9yvFpHab2RjRoFomzTkZu0EQXbWakjdp2iQA7uOBCeprKzCUhloNI5ZSyk+MfoZcBU0wmIfY4YE2z2WU5LTj9DhBp+s5yVLmYEkoYsy5YpAUVNL959v9grgYJVyOZYMEMAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet embedding space\" title=\"MobileNet embedding space\" src=\"/static/d771cbda799648334d423814d2e169fb/b7c40/ResNet101V2-layer4-v2.png\" srcset=\"/static/d771cbda799648334d423814d2e169fb/e4891/ResNet101V2-layer4-v2.png 240w,\n/static/d771cbda799648334d423814d2e169fb/0ce91/ResNet101V2-layer4-v2.png 480w,\n/static/d771cbda799648334d423814d2e169fb/b7c40/ResNet101V2-layer4-v2.png 960w,\n/static/d771cbda799648334d423814d2e169fb/79582/ResNet101V2-layer4-v2.png 1064w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 9: Train Emb. space for ResNet101 base Siamese Network</figcaption>\n</figure>\n<p>In this case, our embeddings look more like the Fig. 4. Results are worse than MobileNetV2 but on a similar level with a basic classifier.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEzUlEQVQ4y6VUXUxbZRg+tKy4AS3Q0h9oC5SVDZbNuQlz003qhK4rp+enPeWnlLa0tNBCT8voSoEhvwW3MS7mmBdLFhOXzMTEG6+M3ui2c6HJjNFh4sWMyS680plo9KT19Tvn1J8rY+KXvPm+853zPOd53vf9PuwO3MWEcf6qvWxgKCEfPr0pY0xj8v7WuJw9tiXTVBuVVXtU2qpypbbuGZ1q7aW7MsYyJndUsrJvdhvLvv1OWybgnz7VYm1tbSIXdhM2sP8zAJR/P8z9mJEJs/cjgugng2/RlvnrLnVgh9KN7VDmxA7zwsoN2hi/QWrC0mxK3KD04VtnsGjm+6+bbD89MToE/O8/GGWiwsm5Fbmw4R7wXaOyV+AcvQW0KgJeQxI8+ino75gHRpeQ9sxpYBpZ8KHZhk189dmXTdnPHxtFe09+NsqtViuGTWmuiYRObHjt/Ctz0LO5+EtflY+nzZO8+/AsTzVN8bjSz+OqER4p5Im60G9ucxx6sei9W592Je887swK+NuPOstFy/6Di+ICt8TXBwwpcCRXeBueBtqcAvLgDBCNccBrQ4CrAkC1sOCqDRXdDXGwYyNc9h7J5r4g5wX8wi4tKYx17YgKXe3Tee/+LPSQa7z91RzQJ5eBbk6Cuz0DpGEc+qr9QOhjQOqiRU8zIpaPcp4Pxlj648iigB/cTUqE0eZVkbBPG8p7jejD41kerw7DUGIUvGcngWqdBaohBoR6FEh9FFzqsKiQlAW4E+9mUl0f5mYFfOf7GclyRD8n5bDGt+5tSAJ1bJ7HK/3AdEbgbHoVqPY0uGpCyOoIkA3jQFnYoqclDe49Ac56e4m1vLOyIOBb39uQi1U+XUsoRIX1oTVPfRyoQ1leAOO1CfB0RyCdowBXj0mWNYLKWJFpy4B7b5Br2c6zpjc3Lwl409vXJcITemeFsEG2Jlf7m2aAPJzlCUMU6PaL4EHF8ERYwIPTQFcGUVGSIqHbhHK7L8hZlzZTLVvbomXLxmXJ8gVzqW3qhvOMNgFkx0UerwmI+SK0EbDrZ2Ap0wNTfU6wK8aAqg8WqXr0Tj7AHbpwmT2YuyIqbF++KSkM10s5dFQP5t3qCSCPX+KFNnGhIhCmONAdOWCMAehOZ8F+AFW7CrUNqjKx188dDb/OHklcFXP4bLJkeaox/2dR8gw6Ga5Tr/F9Sr/QHkCYUZVPLSGlcXBYYuBczMBg80iR0sTRKRrnTvZfSXUNbeUEfNdAyfKw7kK5RDi8zjSwQB7J8c5KH5DIrmCbtk4DpUfWK8Jw8ZwHdmZtRYcK/axikOvuzbNn7OtiH9oc25LCAeO0SNinCa4yHQtAPrfwK64cKRDGeIEwxApUc7Lgqg0WcHWoQFVFCozXzzOxcSDKh+73Hr2U7OlcmhPwvc+vSIR+/YxEWOO/3G9IA/HiMpB16CKwoio3pcF7IAseQwIotOdpRv2nmwRfchKcxPjDXjU7c66JFRXaTaxkecSUlXKo8Y+TLVOPnC8vfeJUDnOkceIBrglxbmv6PqGPPsDrAhxtTXFk48R9oj318MT2G7dsupC3V+Hziwr3+eV/XbClIfyhCkUliorSei+KGhSq0lp4tweFohTiIAH+2018fG4/JleUVSuq5Yp/+872j/Ufnx3tKiSnVxIAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet embedding space\" title=\"MobileNet embedding space\" src=\"/static/12fd09fff5b135717b34cdc5694452b8/b7c40/conf_matrix_ResNet101V2.png\" srcset=\"/static/12fd09fff5b135717b34cdc5694452b8/e4891/conf_matrix_ResNet101V2.png 240w,\n/static/12fd09fff5b135717b34cdc5694452b8/0ce91/conf_matrix_ResNet101V2.png 480w,\n/static/12fd09fff5b135717b34cdc5694452b8/b7c40/conf_matrix_ResNet101V2.png 960w,\n/static/12fd09fff5b135717b34cdc5694452b8/5a2b3/conf_matrix_ResNet101V2.png 1440w,\n/static/12fd09fff5b135717b34cdc5694452b8/eff2e/conf_matrix_ResNet101V2.png 1920w,\n/static/12fd09fff5b135717b34cdc5694452b8/cc694/conf_matrix_ResNet101V2.png 3000w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 10: Confusion Matrix for pig classification (ResNet101 base Siamese Network)</figcaption>\n</figure>\n<p>A more interesting example is a network trained with <strong>EfficientNetB5</strong> base. This network was the hardest to train (train loss at 0.25 in comparison with 0.06 MobileNetV2 and 0.01 ResNet101). Projecting train examples into embedding space had given a surprising result.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 79.8139534883721%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAABxklEQVQ4y41U7WrbMBS1bNlSIjn+UOMmXps4uC2UpU0CTUJhY3SFwcYYLfQJxt5g78+ZLKtpPbtNflzu1ZV8dHSOZMdxHLwZpM63dwucX05NTUj3Ws/zMJ/Pq/odQANAECkJj7ptEErNfLPfalggm109X5zkGKq0c7OOsf3QrRnk6QhZPIRr+4Hv4+nnPR5+3INzhn0nciZliTCKdo312RLL8qo+0n96eXZT16syMVHMSjDGX9ZdLld4+P0Hm0+fTWMUZxBBzxjiaXbuK/ZVMJ/itizAg6BxslfhIFZHyCeTxgSTAjyODaAvGFzJDCOqjfgQyYbOLcB9Qb0XFr1eCiGzlnktQNLltgUKGMPVaoHNdgMpE4Q6xipDyMX7DLmQ4Md5reVIQapBDSg4tr++YjorzDhNU2RqiMfNN5yNC8uUtAGJFrgyoqq/bBfgTNesrw2qmfqaaaTUbv3FaYlRkh2ooX5KJleO+/ZaaFl86y5540G0AHuij+tyipOBMFciEcK8lu7XRPYDJqHA4+oj/n6/Aw189PW9q5ykGni8XuOAW9FscK3hzewUySBsstESDPJ8VzuHHrnrt+Q96/ksS5KAct65/h9y5Vd7JH1qJgAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"EfficientNetB5 embedding space\" title=\"EfficientNetB5 embedding space\" src=\"/static/3239108b561b5440d8fd35a43cb75f51/b7c40/EfficientNetB5-layer3-v2.png\" srcset=\"/static/3239108b561b5440d8fd35a43cb75f51/e4891/EfficientNetB5-layer3-v2.png 240w,\n/static/3239108b561b5440d8fd35a43cb75f51/0ce91/EfficientNetB5-layer3-v2.png 480w,\n/static/3239108b561b5440d8fd35a43cb75f51/b7c40/EfficientNetB5-layer3-v2.png 960w,\n/static/3239108b561b5440d8fd35a43cb75f51/3ef3b/EfficientNetB5-layer3-v2.png 1075w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 11: Train Emb. space for EfficientNetB5 base Siamese Network</figcaption>\n</figure>\n<p>As you can see, we have more than one cluster per class. Data itself is clustered but when calculating a mean value for every class, the end vector is nowhere near any of the positions of the clusters. Besides that, most of them are very close to the center of the space. That results in very low classification score (<strong>F1 = 0.32</strong>).</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 100%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE0klEQVQ4y52USUwbVxjHjZ04AhuzBhswq7FDlqYhKdBShYQkGGPjWb3FYBaD8YLtAS8xiQkhYEYJIakaRCK1QV2lqof21kPV3tJkeqhEpSa9tFWqqqrUW5Gqqh3Z+fpmhi7qIYc+6dO8efb/p//7f59G9sYOyIQ1ELlR5L0bV/henJG7G6IKlymmiB9fl1eq6jTqPZoatbK8prJYV5brelvuPpFUWCvT8m8e1xZ9/4O2SNDv7NTITCaTyJJln0rQ/7sANP+8uD74XC48e9IbuHkr8xZ5eGoDL5+8Q9SF7hD6yB3icGaTqPJv0sYEquQmoQ1s0keS904p4+mfH+n7dn7SDwr6p7/o5aLD0PCrCuHgTOfcra33xyDw8gyQVQy4WpLgrGfAaUiDUzsDrsYEuJtT4NQz4DXNw2lZ6PEXj5ozXz7Rs4L+x1/1ChGYMVwWgTaFM3cx5wEPlf1tSOXnyYMpnj6Y5snaII9VT/J4TYDHqtB5U+wPR1MU+mX+z7a2e+LvfNs5L+jf/a5LArqMiT3CgVUzvHomexXM53O8uTsBhCkJ5IEU4PVhsJePAV49Cfj+KcDrQgVKPwODsgku85Bi5rfxy4L+6hOHwmg0ymSBI1ckh+UjrGv1MhD98zzWlQXq0AUgDbNAts0BVjUBuDYAJIKT+kiB1kfBLhvlHJ8GGfp+QAR6vopJDicPLUnAsmHWspgDvHOOp8umoW9kEWw9aaBb5oDQBYCoC6NnUHBZoGvRvsjHdX94gen8OJMV9F2fXJSA4wcWRKBV7V21Li2DvSPKE6UTYD0+C+fcLOBtDNg1Y0BopwV3gLIsUDrkVOblTG8uM63vLS0K+taP1iXgyUZKKRxg1RM56/IKDB0J8XjpONC1KKeeFPROLQBega5c6UewKenKjXGg5F6u9fY1puG1tUuCvvH1XWC3rm+fcGCvGFsJsn6gO6Z4u2ocyOYYkOiKA44cmC0o20rUFPRO1EdQU2JA7/NxxpUbTOu1Ncnhxl0JGG+/JV55UOVhLVEW7MdmeQw5RN0EvDYIVE0Eer3z0H8uCZQuCkR7qiDkSpWMcofT60z7pfUFQd++eHs3Q+Pi38C+NAvWg2EeU4+LeRENqIwJsFaOQt/5HNgMM6gx4QLdxABZ4uM6gjeZoxEJ+Hx84y+HWRE4UOxm46wHnEfH0ACHxbGhWmeBQC6xsnEgDiWhz70CuCFWcNQipyof95JrjenyrosZdntv7jo0BMXBthR7VpfWMDjfMcrbSiaAaokD0RgFDHUXdRbIikmwdM7CqeErBUqLxqZkhDttZpneAVacw9ODr0iD7WqLikCbZniFWF4A7ET0d6I6lCfqw3lcH86jzPKou3lMG8gTxb78GVuaP+vKAiZ3PjB3LMbPdV4RHZpfWJEc+trTInBIM7JGX18F/NgMUGo/OA5kwNE8Bw7UUUcjA3R9FKj9qEnaCFgCLJztT2wPVkdTA03xq2IPGhJ7RKDbkBAztKjcoVNzl762HYvet6t9HPHchYd4S4wj9OEHdl3gIV4X5IYqxjhcN/0AN8a3e1Lr987qJl1m5bBP0PeXjCj++50UnKpRqVDt290XoypHVba7F37bi0q5W+LqB+kDLWb4rHUyoZMplEWle9Vy5bP+1/uv/Z9+ivP/D+Cg1gAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"MobileNet embedding space\" title=\"MobileNet embedding space\" src=\"/static/8d9e861f5b01b5d059a2e5427a8aaa98/b7c40/conf_matrix_EfficientNetB5.png\" srcset=\"/static/8d9e861f5b01b5d059a2e5427a8aaa98/e4891/conf_matrix_EfficientNetB5.png 240w,\n/static/8d9e861f5b01b5d059a2e5427a8aaa98/0ce91/conf_matrix_EfficientNetB5.png 480w,\n/static/8d9e861f5b01b5d059a2e5427a8aaa98/b7c40/conf_matrix_EfficientNetB5.png 960w,\n/static/8d9e861f5b01b5d059a2e5427a8aaa98/5a2b3/conf_matrix_EfficientNetB5.png 1440w,\n/static/8d9e861f5b01b5d059a2e5427a8aaa98/eff2e/conf_matrix_EfficientNetB5.png 1920w,\n/static/8d9e861f5b01b5d059a2e5427a8aaa98/cc694/conf_matrix_EfficientNetB5.png 3000w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 12: Confusion Matrix for pig classification (EfficientNetB5 base Siamese Network)</figcaption>\n</figure>\n<p>Most of the examples are assigned to one class and that class just happens to be the furthest from the space center :) Even if the version with EfficientNet base wasn’t working as expected, it gave us knowledge on how that approach might work with the different types of networks.</p>\n<h3 id=\"why-this-approach-is-better\"><a href=\"#why-this-approach-is-better\" aria-label=\"why this approach is better permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why this approach is better?</h3>\n<ul>\n<li>Less computation in production, you’re calculating the mean for the class once and just comparing your example with all the means.</li>\n<li>You can easily add a new class just by computing a bunch of images from that class and calculating another mean embedding</li>\n<li>With some additional work, you might have an “outliers” class.</li>\n</ul>\n<h4 id=\"what-about-more-animals\"><a href=\"#what-about-more-animals\" aria-label=\"what about more animals permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What about more animals?</h4>\n<p>This network works with 64D space and it might be beneficial to add more dimensions when dealing with more complex datasets (thousands of subjects).</p>\n<h3 id=\"how-did-it-work-with-the-whole-system\"><a href=\"#how-did-it-work-with-the-whole-system\" aria-label=\"how did it work with the whole system permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How did it work with the whole system?</h3>\n<p>That is a difficult part. I’ve mentioned that <strong>the first part of the system is a detection network</strong>. Siamese Network has to deal with cropped images from that detection and not with nicely cropped train/test dataset.</p>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 927px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 56.742179072276166%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAA9hAAAPYQGoP6dpAAADHUlEQVQozxWSTUzbBQDFe/DsQa8mM5OwDYUB29oCpe2/pbSs0Ha2UAqytYOu3wVK6XfXwTpoa0kKLWDntiyGTcGJ+4iRGBRCWJYYo0tMdGGHbbdpNO5gTDz9rNd3eHnv/Z7oy4cv2T/4l0cH/7D/y1/sPP6db374jfv7L7jz3QGbO0+58+1T7u4+Y2P7SU1/zoO953z16Bnbj1+y9+QVu7++YuunP/ls5wWiytou5RubRJKzjLmCuMdTeCNFvNESvliZYGIVT2qeYa+dodEw9lCec8kgo+Ec7kSZRGWLhY2fuf3wD9a//xuR9FQbTe81oFG3MWLrw+u0Egk5uJjwMZsJcTkdYTI2zJhPRzTq56zdzHjuLKVrZcKROVoVHdS/285xiZ7TA35EEqkctVqFyzFIYsrJbMrLh9kIxfkIC/k4y6UsyYSbdOIS6xv3GQ9YSc1Eufr5EvlKhm7dCQ7Vvc4bb77G4UNvIZIpNBj0vYynTOSWQsTCF5iO9LMwP83SQppqpUA0bKNSWWVj8y7jnl6S0QDZ4jw31zYJTSXQ2NqxOIZpbD6KSKk6jam/m3Clh3w1xuLHBabmDEwFzSyWplkpZ7gyM8m9e3usLGVJhy3EJoZxXrCRzFzBaFSj0soZGYshlp3631DLiGOIlWqFi8k4RksLn9xaq22XJuQ3U12d4epHOW5eL7FYcJOfcZCYGOGcrQeDToxe24GglHDs6Ns0NjUjUgha7GPnuX7jNl+sbzMw0MVELcGDr38k7B+hOOdjuRKimLdTzDqZiQ0S8poZNCkw9Mjo1WtQKmU1sMc43tRSS9hloE0uQxAUhAJxrENmWsUNBHzTFHMFchkXlwtDROJG5i+FyNeAeV19WE0qzvTp6O7W0tzSSv2RBurqjiASt6voFKQYBrQoVBI6NRKkGjUyaT2WM1ICHjNBt41y6Rq31rdYrn6KwSigFSToNCoUClWt6kkOv9NIs1iDqEUs0CEX8E160OmluPzDnJQb6FJ20G/U4LCZ8NS+6fd+gHPUisNuwfK+jo62NrrUaro0Ojo7BZpPyLGdj/Mfz2P3miFhfuoAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Detector output\" title=\"Detector output\" src=\"/static/40a418aa2dfd1eb1be64b3c84757ef46/a406e/out.png\" srcset=\"/static/40a418aa2dfd1eb1be64b3c84757ef46/e4891/out.png 240w,\n/static/40a418aa2dfd1eb1be64b3c84757ef46/0ce91/out.png 480w,\n/static/40a418aa2dfd1eb1be64b3c84757ef46/a406e/out.png 927w\" sizes=\"(max-width: 927px) 100vw, 927px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 13: Detection output</figcaption>\n</figure>\n<figure class=\"image\">\n  <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 960px;\">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 64.28571428571428%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAC4jAAAuIwF4pT92AAAEBElEQVQ4y12Sa0yaBxSGv92ydc71klqa1s7irVG0+oEKiqAioHgpgvIVURREGK3EaqUiKrWCtdZLFQpfqsX7Da0XrHXGlZq222I2F7tlf7YsIVnSmS1Ls+zHks1POJMm24+d5OTNOXny/nlfxLPxArd2d4JOLSTqtRIw6VX+no56wK3XCfudDszUaPRWYEKQlfB9Oq2KWF35DNbdz1gIgpzVqkp9XWYLoZJhf0kKM+GyuoSLWNqM+MyEC6Ym3YROUw6a8jx/q14F9r4WYmZiDOtovu119DmhTCz0paIRRKuhFmYnRln5uSJyb5cVBm2T0N/p8HW0dgIjLoyHyCUc3DZwG7Z3dokhfBIuKUr8NVUiuNWmI+ZnhrER3OXdfLwDpkazLzU+klCV5YNRL2MNDq6Th+8/gAnnIiy5NvZd059ARZGMh4jz0vA6rQhm5laI7Re/wc0bHX5VaTYY6+TE9DiOzY6ved3zG+C8N+/LpKOEpIAB3ZZLLC5XQrbbxmB8ZBmmhhf23UtPocuM8xAem4rrNCJQyi8QT7Z+As+zH/xa+QWoq8ojrqilWH/fiHd6dBkWHjz1YUKM4DATocWgYR0+GkRuNVrgpnkABu1T+4vzHnCvbPGQ6MhwB0+QA8lJ8XvKSoX/82/+9DnujEOzrmxPVy3CZMWV3gadAay9Tl9N9VXiXFQE0Fl0FikkhBwbQwFxQYnvY0XNXou+HSbHHvKQyLMnHMwsAbCy+EQaGuZXy5V+k6EFzMZaQqctxeJjE36kU5OAy+YTAm7+33QqColJ1PSTpNDw0NOhQImJgySUBlmpbMhMy85BElE6zsy+CAwm289hUoCXfg5KBNSDcAr9KkVxKYPO+iWDzQFGSjrExsRCMiMLUjJkHHW1+IxYIv01g83dpaHJL9EE2qvE81Q2Qk3JDk9iCpMzeEXn2dSwRCEnLkGaQ0EL6cco7iUbCZMWrxYX8b+qViu+UCjlzxkswdcRcRz0oIcIAAQd7Ht8geYQM4V5mJOZ+3bgj0RFHzkdHISE09LY5I9ikslRzApyGtYTIb48RUb+N90W+Yf5nDMkOi3y5MEZcuoIckJfW3zsP+D4qbD3CwrEu416E/R034e2djtcqbOAUt0G6fzK7wJMk8n6VkBlF3NDWppqvx2653w1N7u+i9tGd3VVmt+lhVlbIgHlg389DzUael96Ng8q8/h7eLTyJczPPQdzuwNoCTGvDfv7ht8MqKgQJQ0PTfyx6dmB5YVPYXXZA0/WtsHUcPXn6Agk+LWb8Vr2Ow0NTXfvDrgWTfrrs30Wq6vdeGs2hUZdJoUEdwWYMqzojYA2X6sPvtFs6FKWiYdEeUy8XJqLz0yvOadGXe3Hg5F3A8w/rKHUx0rEsKUAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"Cropped output\" title=\"Cropped output\" src=\"/static/b41856fe322f5296c078138b42090991/b7c40/cropped.png\" srcset=\"/static/b41856fe322f5296c078138b42090991/e4891/cropped.png 240w,\n/static/b41856fe322f5296c078138b42090991/0ce91/cropped.png 480w,\n/static/b41856fe322f5296c078138b42090991/b7c40/cropped.png 960w,\n/static/b41856fe322f5296c078138b42090991/3fc71/cropped.png 1400w\" sizes=\"(max-width: 960px) 100vw, 960px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n  <figcaption>Figure 14: Detection output cropped (only 5 elements from the output)</figcaption>\n</figure>\n<p>Fig. 13 shows the sample output from the detection network. You can clearly see that some of the bounding boxes are covering more than one animal. Fig. 14 shows some of the cropped images using just that bounding box definition from the detection network. That cropped images are then sent to Siamese Network and compared with the mean embedding. Some of the cropped images are easily classified (i.e. left top) but some of them (i.e. right top) are having multiple animals on the same image. That might cause a network to output embedding base on features from the different animals that we want and in the end, misclassify the result. We’re dealing with cases like that using some custom heuristics, but you should be aware of the problem.</p>\n<h3 id=\"conclusion\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h3>\n<p>Using mean embeddings to classify animals with Siamese Network happens to work very well. We have to watch out for some edge cases (described above) but at least some of them could be fixed with some heuristics. If you’re interested in the project (or just want to check out the code), we’ve published everything on the <a href=\"https://github.com/burnpiro/farm-animal-tracking\">GitHub repository</a>.</p>\n<p>The main project goal was to track and recognize animals on the video and here is a quick sample result.</p>\n<div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 56.25%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem\" > <iframe src=\"https://www.youtube.com/embed/SEw_Tgrrg_E\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen style=\" position: absolute; top: 0; left: 0; width: 100%; height: 100%; \"></iframe> </div>\n<h3 id=\"references\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References:</h3>\n<ul>\n<li>Gregory R. Koch. <em>“Siamese Neural Networks for One-Shot Image Recognition”.</em> 2015 <a href=\"https://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\">https://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf</a></li>\n<li>Leland McInnes, John Healy, and James Melville. <em>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction.</em> 2020 <a href=\"https://arxiv.org/abs/1802.03426\">https://arxiv.org/abs/1802.03426</a></li>\n<li>Stefan Schneider et al. <em>Similarity Learning Networks for Animal Individual Re-Identification</em> - Beyondthe Capabilities of a Human Observer. Feb. 2019.</li>\n<li>Tensorflow.Embedding Projector - Web Access Jan 2021 <a href=\"https://projector.tensorflow.org/\">https://projector.tensorflow.org/</a></li>\n<li>Mingxing Tan and Quoc V. Le. <em>EfficientNet: Rethinking Model Scaling for Convolutional NeuralNetworks.</em> 2020 <a href=\"https://arxiv.org/abs/1905.11946\">https://arxiv.org/abs/1905.11946</a></li>\n<li>Mark Sandler et al. <em>MobileNetV2: Inverted Residuals and Linear Bottlenecks.</em> 2019 <a href=\"https://arxiv.org/abs/1801.04381\">https://arxiv.org/abs/1801.04381</a></li>\n<li>Kaiming He et al. <em>Deep Residual Learning for Image Recognition.</em> 2015 <a href=\"https://arxiv.org/abs/1512.03385\">https://arxiv.org/abs/1512.03385</a></li>\n<li>Perceptual Systems Research Group - University of Nebraska <a href=\"http://psrg.unl.edu/Projects/Details/12-Animal-Tracking\">http://psrg.unl.edu/Projects/Details/12-Animal-Tracking</a></li>\n<li>S. Schneider, G. Taylor, S. Linquist and S. Kremer <em>Similarity Learning Networks for Animal Individual Re-Identification — Beyond the Capabilities of a Human Observer</em> 2020 <a href=\"https://arxiv.org/abs/1902.09324\">https://arxiv.org/abs/1902.09324</a></li>\n<li>Elad Hoffer, Nir Ailon <em>Deep metric learning using Triplet network</em> 2014 <a href=\"https://arxiv.org/abs/1412.6622\">https://arxiv.org/abs/1412.6622</a></li>\n</ul>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>"}},"pageContext":{"slug":"/2021/02/animal-recognition-with-siamese-networks-and-mean-embeddings"}}}