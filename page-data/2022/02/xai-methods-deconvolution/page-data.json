{"componentChunkName":"component---src-templates-post-template-js","path":"/2022/02/xai-methods-deconvolution","result":{"data":{"markdownRemark":{"fields":{"slug":"/2022/02/xai-methods-deconvolution","tagSlugs":["/tag/machine-learning/","/tag/xai/","/tag/interpretability/"],"readTime":{"text":"9 min read","minutes":8.95}},"frontmatter":{"description":"What is Deconvolution? How it is using CNN structure to make for interpretability?","tags":["Machine Learning","XAI","Interpretability"],"date":"2022-02-21","title":"XAI Methods - Deconvolution"},"html":"<h2 id=\"what-is-deconvolution\" style=\"position:relative;\"><a href=\"#what-is-deconvolution\" aria-label=\"what is deconvolution permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>What is Deconvolution?</h2>\n<p>The idea of <strong><em>Deconvolution</em></strong> <a href=\"https://arxiv.org/abs/1311.2901\">[2]</a> comes from the work of Zeiler et al. <a href=\"https://ieeexplore.ieee.org/document/6126474\">[1]</a> about <strong>Deconvolutional Networks</strong> (<strong><em>deconvnets</em></strong>}. Deconvnets are designed to work similar to convolutional networks but reverse (reversing pooling component, reversing filter component etc.), and they can be trained using an unsupervised approach. In a deconvolutional approach to explaining the model, we are not training a deconvnet but rather probe our CNN with it.</p>\n<p>To reconstruct the activation on a specific layer, we are attaching <strong>deconv layers</strong> to corresponding <strong>CNN layers</strong> (see <a href=\"#figure-1\">Fig. 1</a>). Then an image is passed through the CNN, and the network computes the output. To examine a reconstruction for a given class <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span>, we have to set all activations except the one responsible for predicting class <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>c</mi></mrow><annotation encoding=\"application/x-tex\">c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">c</span></span></span></span> to zero. Then we can propagate through deconvnet layers and pass all the feature maps as inputs to corresponding layers.</p>\n<figure id=\"figure-1\">\n    <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 984px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 62.18749999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABg0lEQVQoz12SiXKCMBRF+f//q1VQtoAmLGFTwqK1h6RlOr0zOm+/973gvS3WdX0+n8643+/8f/3CFYzj+Hq9nL0HPX60kTPGCCGiOO66bpqmeZ7fv8AlWBSl1tqMZk/9NOPb5ixO0lrrZVmMGbM8v4Th4/EgWxRFmqa4WZaVZblamVszThiGJESWUafrGiGQtG3XNA2cBNu2RTYGo6WUfT9szYRYEqr7MCilRJ7fpHQ6KWX53uJ6vRKv61o3zWJBzdZcWChVsDBSKSNhLCh6WIRRRALlJ9+XSiGW7CbbDT5fLokQzML9e1IHJ7vru4/DIUlT+HE9rmXGcZ6nKIpiJgvB/lo3O/MwDAhheZg5SllVuKyzyYaBxE3eEBYnCZKY0NrXckXu2pxDKkkzI9yOBD33jCc/OByP6Nll/wP3gwbxfhB8no7wb9fm1DwM2pSUCUgF+yOYIClEYdPJLaHlKwrOZ0s79UPvud3QsL1t0zCS98B1B2coNsqZWFUVs0gz1AW/ASb5p8n3y4UAAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"deconvnet layer\" title=\"deconvnet layer\" src=\"/static/1ec3edf72fde6dffcbecca6018845119/0b6f4/deconv-layer.png\" srcset=\"/static/1ec3edf72fde6dffcbecca6018845119/72799/deconv-layer.png 320w,\n/static/1ec3edf72fde6dffcbecca6018845119/6af66/deconv-layer.png 640w,\n/static/1ec3edf72fde6dffcbecca6018845119/0b6f4/deconv-layer.png 984w\" sizes=\"(max-width: 984px) 100vw, 984px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n    <figcaption>Figure 1: A deconvnet layer (left) attached to a CNN layer (right), source <a href=\"https://arxiv.org/abs/1311.2901\">[1]</a>.</figcaption>\n</figure>\n<figure id=\"figure-2\">\n    <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 984px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 38.75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB3UlEQVQY00VQTWsTURSdneBPqNCFS7tw60LUpRslLlwoiAsFIZVurIjRBqxFsFpbA0pQ2iTE6hj8KFKLgWpXNe1MzAdJrU3VpJl0Jk0nbzpfb95k5r3nSyh47+Jyzz0H7jmcbdu6bjgIUR9vVNbl1g6l1HXdarUqCILrdjD2VUUCYMd0kef7pml1yb3iZFnekrZNUF/6/PBx9M58fJJC9OjJxKG+voGBI5NTEbMtLaUnItPhhRfje0oT6JplWftiAMAu0H2IhM3KvbSYiUb9ei1fLp8LBAYHg7dDIfH7cqGtjX1ZEaZjbkvVDai124SQrliS6k3pZ7GSSZfmF3L8X0liKEKOmM3+yOVsCNmqA82zlNJW8duvxU8rsy0VMJBgzBXSY79fHV9/f/Lp3NUbsfP87Dh1HUwIy6KtARs6lHiby1OVt4H86xPP312/nwoufk0St2ubo8afjx+GhuIjw8nEzJlTc0f7xcsXIMvQNFEvGORhtSa+4W8OJ+7eSsRfXjybOn1s9coljBDHzoW9Tj9fOpjMBp+l1kbDGw9GG8X8dmvX67D2ZEWRG0qmaR/mCwdiq9cifHkktBYOWbUqRwhm/6sOaliOTQndLxYI6Q1mDfc4VIOwbkCD/Cf9A8zihKmmCUK7AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"unpulling layer\" title=\"unpulling layer\" src=\"/static/a2f1e011c9815cd32409d0903104484b/0b6f4/deconv-layer-unpooling.png\" srcset=\"/static/a2f1e011c9815cd32409d0903104484b/72799/deconv-layer-unpooling.png 320w,\n/static/a2f1e011c9815cd32409d0903104484b/6af66/deconv-layer-unpooling.png 640w,\n/static/a2f1e011c9815cd32409d0903104484b/0b6f4/deconv-layer-unpooling.png 984w\" sizes=\"(max-width: 984px) 100vw, 984px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n    <figcaption>Figure 2: The unpooling layer, source <a href=\"https://arxiv.org/abs/1311.2901\">[1]</a>.</figcaption>\n</figure>\n<p>To calculate the reconstruction, deconvnet layer has to be able to reverse operations performed by the CNN layers. Authors designed specific components to compote the reverse operations done by CNN layers:</p>\n<h3 id=\"filtering\" style=\"position:relative;\"><a href=\"#filtering\" aria-label=\"filtering permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Filtering</h3>\n<p><em>Filtering</em> in the original CNN computes <strong><em>feature maps</em></strong> using learned filters. Reversing that operation requires the use of a transposed version of the same filters. Those transposed filters are then applied to the <strong><em>Rectified Unpooled Maps</em></strong>.</p>\n<h3 id=\"rectification\" style=\"position:relative;\"><a href=\"#rectification\" aria-label=\"rectification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Rectification</h3>\n<p><em>Rectification</em> uses the same <em>ReLU</em> non-linearity <a href=\"https://www.nature.com/articles/35016072\">[4]</a> to compute <em>Rectified Unpooled Maps</em> as it is used in CNN. It is simply just rectifying the values and propagate only non-negative ones to the <em>filtering</em> layer.</p>\n<h3 id=\"unpooling\" style=\"position:relative;\"><a href=\"#unpooling\" aria-label=\"unpooling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Unpooling</h3>\n<p><em>Unpooling</em> corresponds to the <em>Pooling Layer</em> of CNN (see <a href=\"#figure-2\">Fig. 2</a>). The original max-pooling operation is non-invertible, but this approach uses additional variables called <strong><em>switch variables</em></strong>, which are responsible for remembering the locations of the maxima for each pooling region. The unpooling layer uses these variables to make a reconstruction into the same locations as when the pooling was calculated.</p>\n<figure id=\"figure-3\">\n    <span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 57.1875%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAChklEQVQoz3WS2U8TARDG94/Q+OQVTAQ5wlVELJSIFKEUIYEoGokEPPBBIDxgJCIISWkqVFATaDEcBjxqxMTwgCREExIPTDhs2Zbu0d22u93dbtd2u4WKRKe+O5mHmYdfZr75BrG9mBob6Xv/dnLU/KC747p1pP+d7dncS8vs5PCXz0vhcEQKhQKBAI5jBEGQJMnzfCgUkiRpe3sbmZ0ef2ru7r3TUpafqivK0aqzO29fBXLKMvD966dIRA4GgziGe0iP3+93uVxer5cXhHA4nICto497Om+0XdG3XK5qr9dqClTpxw421moX51+JHBsIcDRFETi+tbW1traGbm5CQRKEIAixWAyZnhir02lKC7Ia6nQVxSfLNKcqTmdkHd3Xe7dDkSWGYXEcd7vdTqdzdXV1Y2PDQyUiwLIJePTJ0FlVcm1J3uBN3VhXo7Hl/LXq4syUpMLcNBJzBYMiRXlQFAUMwzAU3aRpmmVZ0KIoCvLQ2K9KPnSxXD3RUUM9b1+3tlUWZedlpuWlJi0tzMty1EOSdvsPmOxEURgOLYgXRTEBG/p6zqhS66tKjK2XZu41NFfknzi8PyM5SavOXf64qCgxL03b7XaHI5EQcDCA4drRaBQZNg/W6Mury4rPFeYeP3JguLlu2dTa1aTX5KTMTFh2duIgj6YpgsAZhgEMduY4DuCEZmN/T2WpWleiHrqlf93bRI0PuC3t3Nz9C5p0Y1/37729n5IEGRJF6V8RiURgYfApHo8jjN/ncqzjLkdc9P4S/bGAX/S6FcGLOR0+mgZLwJuVlW8fFhbe2GzjVssj85DJZDIYDOAc8uf/sbu7C8JkWYaXgK+CI7EMI/C8z+fjOQ62+AsEO9CM9MDZSgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"deconvnet result\" title=\"deconvnet result\" src=\"/static/0cd0213eee31f686e306e08c0b584da8/5a190/deconv-black-and-tan-coonhound.png\" srcset=\"/static/0cd0213eee31f686e306e08c0b584da8/72799/deconv-black-and-tan-coonhound.png 320w,\n/static/0cd0213eee31f686e306e08c0b584da8/6af66/deconv-black-and-tan-coonhound.png 640w,\n/static/0cd0213eee31f686e306e08c0b584da8/5a190/deconv-black-and-tan-coonhound.png 800w\" sizes=\"(max-width: 800px) 100vw, 800px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span>\n    <figcaption>Figure 3: Visualization of the saliency map generated by deconvolution for the class <i>\"black-and-tan-coonhound\"</i>. Image source: <a href=\"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\">Stanford Dogs</a>.</figcaption>\n</figure>\n<p>Propagation through the whole deconvnet gives us a representation of the features from the first layer of the original CNN (the last deconvnet layer corresponds to the first CNN layer). This approach causes the saliency map to feature some biases from the first convolutional layer and the representation looks like a localized edge detector (see <a href=\"#figure-3\">Fig. 3</a>). It usually works better when there is a clear distinction in the feature importance rather than similar values for the whole image.</p>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References:</h3>\n<ol>\n<li>M. D. Zeiler, G. W. Taylor, R. Fergus. <a href=\"https://ieeexplore.ieee.org/document/6126474\">Adaptive deconvolutional networks for mid and high level feature\nlearning</a>, 2011.</li>\n<li>D. Zeiler, R. Fergus. <a href=\"https://arxiv.org/abs/1311.2901\">Visualizing and Understanding Convolutional Networks</a>, 2013.</li>\n<li>A. Khosla, N. Jayadevaprakash, B. Yao, L. Fei-Fei. Stanford dogs dataset. <a href=\"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\">https://www.kaggle.com/jessicali9530/stanford-dogs-dataset</a>, 2019. Accessed: 2021-10-01.</li>\n<li>R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. J. Douglas, S. Seung. <a href=\"https://www.nature.com/articles/35016072\">Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</a>, 2000.</li>\n</ol>\n<style class=\"grvsc-styles\">\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n</style>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h2","properties":{"id":"what-is-deconvolution","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#what-is-deconvolution","ariaLabel":"what is deconvolution permalink","className":["anchor","before"]},"children":[{"type":"element","tagName":"svg","properties":{"ariaHidden":"true","focusable":"false","height":"16","version":"1.1","viewBox":"0 0 16 16","width":"16"},"children":[{"type":"element","tagName":"path","properties":{"fillRule":"evenodd","d":"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"},"children":[]}]}]},{"type":"text","value":"What is Deconvolution?"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"The idea of "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Deconvolution"}]}]},{"type":"text","value":" "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1311.2901"},"children":[{"type":"text","value":"[2]"}]},{"type":"text","value":" comes from the work of Zeiler et al. "},{"type":"element","tagName":"a","properties":{"href":"https://ieeexplore.ieee.org/document/6126474"},"children":[{"type":"text","value":"[1]"}]},{"type":"text","value":" about "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Deconvolutional Networks"}]},{"type":"text","value":" ("},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"deconvnets"}]}]},{"type":"text","value":"}. Deconvnets are designed to work similar to convolutional networks but reverse (reversing pooling component, reversing filter component etc.), and they can be trained using an unsupervised approach. In a deconvolutional approach to explaining the model, we are not training a deconvnet but rather probe our CNN with it."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"To reconstruct the activation on a specific layer, we are attaching "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"deconv layers"}]},{"type":"text","value":" to corresponding "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"CNN layers"}]},{"type":"text","value":" (see "},{"type":"element","tagName":"a","properties":{"href":"#figure-1"},"children":[{"type":"text","value":"Fig. 1"}]},{"type":"text","value":"). Then an image is passed through the CNN, and the network computes the output. To examine a reconstruction for a given class "},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"c"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"c"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.43056em;vertical-align:0em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathdefault"]},"children":[{"type":"text","value":"c"}]}]}]}]},{"type":"text","value":", we have to set all activations except the one responsible for predicting class "},{"type":"element","tagName":"span","properties":{"className":["katex"]},"children":[{"type":"element","tagName":"span","properties":{"className":["katex-mathml"]},"children":[{"type":"element","tagName":"math","properties":{},"children":[{"type":"element","tagName":"semantics","properties":{},"children":[{"type":"element","tagName":"mrow","properties":{},"children":[{"type":"element","tagName":"mi","properties":{},"children":[{"type":"text","value":"c"}]}]},{"type":"element","tagName":"annotation","properties":{"encoding":"application/x-tex"},"children":[{"type":"text","value":"c"}]}]}]}]},{"type":"element","tagName":"span","properties":{"className":["katex-html"],"ariaHidden":"true"},"children":[{"type":"element","tagName":"span","properties":{"className":["base"]},"children":[{"type":"element","tagName":"span","properties":{"className":["strut"],"style":"height:0.43056em;vertical-align:0em;"},"children":[]},{"type":"element","tagName":"span","properties":{"className":["mord","mathdefault"]},"children":[{"type":"text","value":"c"}]}]}]}]},{"type":"text","value":" to zero. Then we can propagate through deconvnet layers and pass all the feature maps as inputs to corresponding layers."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"figure","properties":{"id":"figure-1"},"children":[{"type":"text","value":"\n    "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-wrapper"],"style":"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 984px; "},"children":[{"type":"text","value":"\n      "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-background-image"],"style":"padding-bottom: 62.18749999999999%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsTAAALEwEAmpwYAAABg0lEQVQoz12SiXKCMBRF+f//q1VQtoAmLGFTwqK1h6RlOr0zOm+/973gvS3WdX0+n8643+/8f/3CFYzj+Hq9nL0HPX60kTPGCCGiOO66bpqmeZ7fv8AlWBSl1tqMZk/9NOPb5ixO0lrrZVmMGbM8v4Th4/EgWxRFmqa4WZaVZblamVszThiGJESWUafrGiGQtG3XNA2cBNu2RTYGo6WUfT9szYRYEqr7MCilRJ7fpHQ6KWX53uJ6vRKv61o3zWJBzdZcWChVsDBSKSNhLCh6WIRRRALlJ9+XSiGW7CbbDT5fLokQzML9e1IHJ7vru4/DIUlT+HE9rmXGcZ6nKIpiJgvB/lo3O/MwDAhheZg5SllVuKyzyYaBxE3eEBYnCZKY0NrXckXu2pxDKkkzI9yOBD33jCc/OByP6Nll/wP3gwbxfhB8no7wb9fm1DwM2pSUCUgF+yOYIClEYdPJLaHlKwrOZ0s79UPvud3QsL1t0zCS98B1B2coNsqZWFUVs0gz1AW/ASb5p8n3y4UAAAAAAElFTkSuQmCC'); background-size: cover; display: block;"},"children":[]},{"type":"text","value":"\n  "},{"type":"element","tagName":"img","properties":{"className":["gatsby-resp-image-image"],"alt":"deconvnet layer","title":"deconvnet layer","src":"/static/1ec3edf72fde6dffcbecca6018845119/0b6f4/deconv-layer.png","srcSet":["/static/1ec3edf72fde6dffcbecca6018845119/72799/deconv-layer.png 320w","/static/1ec3edf72fde6dffcbecca6018845119/6af66/deconv-layer.png 640w","/static/1ec3edf72fde6dffcbecca6018845119/0b6f4/deconv-layer.png 984w"],"sizes":["(max-width:","984px)","100vw,","984px"],"style":"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;","loading":"lazy"},"children":[]},{"type":"text","value":"\n    "}]},{"type":"text","value":"\n    "},{"type":"element","tagName":"figcaption","properties":{},"children":[{"type":"text","value":"Figure 1: A deconvnet layer (left) attached to a CNN layer (right), source "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1311.2901"},"children":[{"type":"text","value":"[1]"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"figure","properties":{"id":"figure-2"},"children":[{"type":"text","value":"\n    "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-wrapper"],"style":"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 984px; "},"children":[{"type":"text","value":"\n      "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-background-image"],"style":"padding-bottom: 38.75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB3UlEQVQY00VQTWsTURSdneBPqNCFS7tw60LUpRslLlwoiAsFIZVurIjRBqxFsFpbA0pQ2iTE6hj8KFKLgWpXNe1MzAdJrU3VpJl0Jk0nbzpfb95k5r3nSyh47+Jyzz0H7jmcbdu6bjgIUR9vVNbl1g6l1HXdarUqCILrdjD2VUUCYMd0kef7pml1yb3iZFnekrZNUF/6/PBx9M58fJJC9OjJxKG+voGBI5NTEbMtLaUnItPhhRfje0oT6JplWftiAMAu0H2IhM3KvbSYiUb9ei1fLp8LBAYHg7dDIfH7cqGtjX1ZEaZjbkvVDai124SQrliS6k3pZ7GSSZfmF3L8X0liKEKOmM3+yOVsCNmqA82zlNJW8duvxU8rsy0VMJBgzBXSY79fHV9/f/Lp3NUbsfP87Dh1HUwIy6KtARs6lHiby1OVt4H86xPP312/nwoufk0St2ubo8afjx+GhuIjw8nEzJlTc0f7xcsXIMvQNFEvGORhtSa+4W8OJ+7eSsRfXjybOn1s9coljBDHzoW9Tj9fOpjMBp+l1kbDGw9GG8X8dmvX67D2ZEWRG0qmaR/mCwdiq9cifHkktBYOWbUqRwhm/6sOaliOTQndLxYI6Q1mDfc4VIOwbkCD/Cf9A8zihKmmCUK7AAAAAElFTkSuQmCC'); background-size: cover; display: block;"},"children":[]},{"type":"text","value":"\n  "},{"type":"element","tagName":"img","properties":{"className":["gatsby-resp-image-image"],"alt":"unpulling layer","title":"unpulling layer","src":"/static/a2f1e011c9815cd32409d0903104484b/0b6f4/deconv-layer-unpooling.png","srcSet":["/static/a2f1e011c9815cd32409d0903104484b/72799/deconv-layer-unpooling.png 320w","/static/a2f1e011c9815cd32409d0903104484b/6af66/deconv-layer-unpooling.png 640w","/static/a2f1e011c9815cd32409d0903104484b/0b6f4/deconv-layer-unpooling.png 984w"],"sizes":["(max-width:","984px)","100vw,","984px"],"style":"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;","loading":"lazy"},"children":[]},{"type":"text","value":"\n    "}]},{"type":"text","value":"\n    "},{"type":"element","tagName":"figcaption","properties":{},"children":[{"type":"text","value":"Figure 2: The unpooling layer, source "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1311.2901"},"children":[{"type":"text","value":"[1]"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"To calculate the reconstruction, deconvnet layer has to be able to reverse operations performed by the CNN layers. Authors designed specific components to compote the reverse operations done by CNN layers:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"filtering","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#filtering","ariaLabel":"filtering permalink","className":["anchor","before"]},"children":[{"type":"element","tagName":"svg","properties":{"ariaHidden":"true","focusable":"false","height":"16","version":"1.1","viewBox":"0 0 16 16","width":"16"},"children":[{"type":"element","tagName":"path","properties":{"fillRule":"evenodd","d":"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"},"children":[]}]}]},{"type":"text","value":"Filtering"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Filtering"}]},{"type":"text","value":" in the original CNN computes "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"feature maps"}]}]},{"type":"text","value":" using learned filters. Reversing that operation requires the use of a transposed version of the same filters. Those transposed filters are then applied to the "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Rectified Unpooled Maps"}]}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"rectification","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#rectification","ariaLabel":"rectification permalink","className":["anchor","before"]},"children":[{"type":"element","tagName":"svg","properties":{"ariaHidden":"true","focusable":"false","height":"16","version":"1.1","viewBox":"0 0 16 16","width":"16"},"children":[{"type":"element","tagName":"path","properties":{"fillRule":"evenodd","d":"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"},"children":[]}]}]},{"type":"text","value":"Rectification"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Rectification"}]},{"type":"text","value":" uses the same "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"ReLU"}]},{"type":"text","value":" non-linearity "},{"type":"element","tagName":"a","properties":{"href":"https://www.nature.com/articles/35016072"},"children":[{"type":"text","value":"[4]"}]},{"type":"text","value":" to compute "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Rectified Unpooled Maps"}]},{"type":"text","value":" as it is used in CNN. It is simply just rectifying the values and propagate only non-negative ones to the "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"filtering"}]},{"type":"text","value":" layer."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"unpooling","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#unpooling","ariaLabel":"unpooling permalink","className":["anchor","before"]},"children":[{"type":"element","tagName":"svg","properties":{"ariaHidden":"true","focusable":"false","height":"16","version":"1.1","viewBox":"0 0 16 16","width":"16"},"children":[{"type":"element","tagName":"path","properties":{"fillRule":"evenodd","d":"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"},"children":[]}]}]},{"type":"text","value":"Unpooling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Unpooling"}]},{"type":"text","value":" corresponds to the "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Pooling Layer"}]},{"type":"text","value":" of CNN (see "},{"type":"element","tagName":"a","properties":{"href":"#figure-2"},"children":[{"type":"text","value":"Fig. 2"}]},{"type":"text","value":"). The original max-pooling operation is non-invertible, but this approach uses additional variables called "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"switch variables"}]}]},{"type":"text","value":", which are responsible for remembering the locations of the maxima for each pooling region. The unpooling layer uses these variables to make a reconstruction into the same locations as when the pooling was calculated."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"figure","properties":{"id":"figure-3"},"children":[{"type":"text","value":"\n    "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-wrapper"],"style":"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; "},"children":[{"type":"text","value":"\n      "},{"type":"element","tagName":"span","properties":{"className":["gatsby-resp-image-background-image"],"style":"padding-bottom: 57.1875%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAChklEQVQoz3WS2U8TARDG94/Q+OQVTAQ5wlVELJSIFKEUIYEoGokEPPBBIDxgJCIISWkqVFATaDEcBjxqxMTwgCREExIPTDhs2Zbu0d22u93dbtd2u4WKRKe+O5mHmYdfZr75BrG9mBob6Xv/dnLU/KC747p1pP+d7dncS8vs5PCXz0vhcEQKhQKBAI5jBEGQJMnzfCgUkiRpe3sbmZ0ef2ru7r3TUpafqivK0aqzO29fBXLKMvD966dIRA4GgziGe0iP3+93uVxer5cXhHA4nICto497Om+0XdG3XK5qr9dqClTpxw421moX51+JHBsIcDRFETi+tbW1traGbm5CQRKEIAixWAyZnhir02lKC7Ia6nQVxSfLNKcqTmdkHd3Xe7dDkSWGYXEcd7vdTqdzdXV1Y2PDQyUiwLIJePTJ0FlVcm1J3uBN3VhXo7Hl/LXq4syUpMLcNBJzBYMiRXlQFAUMwzAU3aRpmmVZ0KIoCvLQ2K9KPnSxXD3RUUM9b1+3tlUWZedlpuWlJi0tzMty1EOSdvsPmOxEURgOLYgXRTEBG/p6zqhS66tKjK2XZu41NFfknzi8PyM5SavOXf64qCgxL03b7XaHI5EQcDCA4drRaBQZNg/W6Mury4rPFeYeP3JguLlu2dTa1aTX5KTMTFh2duIgj6YpgsAZhgEMduY4DuCEZmN/T2WpWleiHrqlf93bRI0PuC3t3Nz9C5p0Y1/37729n5IEGRJF6V8RiURgYfApHo8jjN/ncqzjLkdc9P4S/bGAX/S6FcGLOR0+mgZLwJuVlW8fFhbe2GzjVssj85DJZDIYDOAc8uf/sbu7C8JkWYaXgK+CI7EMI/C8z+fjOQ62+AsEO9CM9MDZSgAAAABJRU5ErkJggg=='); background-size: cover; display: block;"},"children":[]},{"type":"text","value":"\n  "},{"type":"element","tagName":"img","properties":{"className":["gatsby-resp-image-image"],"alt":"deconvnet result","title":"deconvnet result","src":"/static/0cd0213eee31f686e306e08c0b584da8/5a190/deconv-black-and-tan-coonhound.png","srcSet":["/static/0cd0213eee31f686e306e08c0b584da8/72799/deconv-black-and-tan-coonhound.png 320w","/static/0cd0213eee31f686e306e08c0b584da8/6af66/deconv-black-and-tan-coonhound.png 640w","/static/0cd0213eee31f686e306e08c0b584da8/5a190/deconv-black-and-tan-coonhound.png 800w"],"sizes":["(max-width:","800px)","100vw,","800px"],"style":"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;","loading":"lazy"},"children":[]},{"type":"text","value":"\n    "}]},{"type":"text","value":"\n    "},{"type":"element","tagName":"figcaption","properties":{},"children":[{"type":"text","value":"Figure 3: Visualization of the saliency map generated by deconvolution for the class "},{"type":"element","tagName":"i","properties":{},"children":[{"type":"text","value":"\"black-and-tan-coonhound\""}]},{"type":"text","value":". Image source: "},{"type":"element","tagName":"a","properties":{"href":"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset"},"children":[{"type":"text","value":"Stanford Dogs"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Propagation through the whole deconvnet gives us a representation of the features from the first layer of the original CNN (the last deconvnet layer corresponds to the first CNN layer). This approach causes the saliency map to feature some biases from the first convolutional layer and the representation looks like a localized edge detector (see "},{"type":"element","tagName":"a","properties":{"href":"#figure-3"},"children":[{"type":"text","value":"Fig. 3"}]},{"type":"text","value":"). It usually works better when there is a clear distinction in the feature importance rather than similar values for the whole image."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{"id":"references","style":"position:relative;"},"children":[{"type":"element","tagName":"a","properties":{"href":"#references","ariaLabel":"references permalink","className":["anchor","before"]},"children":[{"type":"element","tagName":"svg","properties":{"ariaHidden":"true","focusable":"false","height":"16","version":"1.1","viewBox":"0 0 16 16","width":"16"},"children":[{"type":"element","tagName":"path","properties":{"fillRule":"evenodd","d":"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"},"children":[]}]}]},{"type":"text","value":"References:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"M. D. Zeiler, G. W. Taylor, R. Fergus. "},{"type":"element","tagName":"a","properties":{"href":"https://ieeexplore.ieee.org/document/6126474"},"children":[{"type":"text","value":"Adaptive deconvolutional networks for mid and high level feature\nlearning"}]},{"type":"text","value":", 2011."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"D. Zeiler, R. Fergus. "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1311.2901"},"children":[{"type":"text","value":"Visualizing and Understanding Convolutional Networks"}]},{"type":"text","value":", 2013."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"A. Khosla, N. Jayadevaprakash, B. Yao, L. Fei-Fei. Stanford dogs dataset. "},{"type":"element","tagName":"a","properties":{"href":"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset"},"children":[{"type":"text","value":"https://www.kaggle.com/jessicali9530/stanford-dogs-dataset"}]},{"type":"text","value":", 2019. Accessed: 2021-10-01."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. J. Douglas, S. Seung. "},{"type":"element","tagName":"a","properties":{"href":"https://www.nature.com/articles/35016072"},"children":[{"type":"text","value":"Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit"}]},{"type":"text","value":", 2000."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"style","properties":{"className":["grvsc-styles"]},"children":[{"type":"text","value":"\n  .grvsc-container {\n    overflow: auto;\n    position: relative;\n    -webkit-overflow-scrolling: touch;\n    padding-top: 1rem;\n    padding-top: var(--grvsc-padding-top, var(--grvsc-padding-v, 1rem));\n    padding-bottom: 1rem;\n    padding-bottom: var(--grvsc-padding-bottom, var(--grvsc-padding-v, 1rem));\n    border-radius: 8px;\n    border-radius: var(--grvsc-border-radius, 8px);\n    font-feature-settings: normal;\n    line-height: 1.4;\n  }\n  \n  .grvsc-code {\n    display: table;\n  }\n  \n  .grvsc-line {\n    display: table-row;\n    box-sizing: border-box;\n    width: 100%;\n    position: relative;\n  }\n  \n  .grvsc-line > * {\n    position: relative;\n  }\n  \n  .grvsc-gutter-pad {\n    display: table-cell;\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  .grvsc-gutter {\n    display: table-cell;\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter::before {\n    content: attr(data-content);\n  }\n  \n  .grvsc-source {\n    display: table-cell;\n    padding-left: 1.5rem;\n    padding-left: var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem));\n    padding-right: 1.5rem;\n    padding-right: var(--grvsc-padding-right, var(--grvsc-padding-h, 1.5rem));\n  }\n  \n  .grvsc-source:empty::after {\n    content: ' ';\n    -webkit-user-select: none;\n    -moz-user-select: none;\n    user-select: none;\n  }\n  \n  .grvsc-gutter + .grvsc-source {\n    padding-left: 0.75rem;\n    padding-left: calc(var(--grvsc-padding-left, var(--grvsc-padding-h, 1.5rem)) / 2);\n  }\n  \n  /* Line transformer styles */\n  \n  .grvsc-has-line-highlighting > .grvsc-code > .grvsc-line::before {\n    content: ' ';\n    position: absolute;\n    width: 100%;\n  }\n  \n  .grvsc-line-diff-add::before {\n    background-color: var(--grvsc-line-diff-add-background-color, rgba(0, 255, 60, 0.2));\n  }\n  \n  .grvsc-line-diff-del::before {\n    background-color: var(--grvsc-line-diff-del-background-color, rgba(255, 0, 20, 0.2));\n  }\n  \n  .grvsc-line-number {\n    padding: 0 2px;\n    text-align: right;\n    opacity: 0.7;\n  }\n  \n"}]}],"data":{"quirksMode":false}}}},"pageContext":{"slug":"/2022/02/xai-methods-deconvolution"}},"staticQueryHashes":["2857265606","3631363646"]}